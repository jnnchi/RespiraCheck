{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gYscyCg6b7rZ"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import collections\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import random_split, DataLoader, TensorDataset, WeightedRandomSampler\n",
        "from torchvision import transforms\n",
        "import io\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0AUU9kYcUAv",
        "outputId": "15f3ffd7-6535-4340-873e-aa1e18cde29d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "['Cough Data']\n",
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "print(os.listdir('/content/gdrive/MyDrive/RespiraCheck'))\n",
        "print(os.path.exists('/content/gdrive/MyDrive/RespiraCheck/Cough Data/spectrograms/positive'))\n",
        "print(os.path.exists('/content/gdrive/MyDrive/RespiraCheck/Cough Data/spectrograms/negative'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tbLR_B94cdPR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    \"\"\"A convolutional neural network model based on EfficientNet for spectrogram processing.\"\"\"\n",
        "\n",
        "    def __init__(self, dropout: float = 0.0):\n",
        "        \"\"\"Initializes the CNNModel using EfficientNet-B0 with an optional dropout layer.\n",
        "\n",
        "        Args:\n",
        "            dropout (float): Dropout probability before the final classification layer.\n",
        "        \"\"\"\n",
        "        super(CNNModel, self).__init__()\n",
        "\n",
        "        # Load EfficientNet-B0 with pre-trained weights\n",
        "        self.efficientnet = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
        "\n",
        "        # Get the number of features from the last layer of EfficientNet\n",
        "        num_features = self.efficientnet.classifier[1].in_features\n",
        "\n",
        "        # Replace the classifier with a new sequence including Dropout and FC layer\n",
        "        self.efficientnet.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),  # Dropout before classification layer\n",
        "            nn.Linear(num_features, 1)  # Binary classification output\n",
        "        )\n",
        "\n",
        "        # Initialize the new FC layer weights\n",
        "        nn.init.normal_(self.efficientnet.classifier[1].weight, mean=0.0, std=0.01)\n",
        "        nn.init.zeros_(self.efficientnet.classifier[1].bias)\n",
        "\n",
        "    def forward(self, spectrogram: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Defines the forward pass for EfficientNet with dropout.\n",
        "\n",
        "        Args:\n",
        "            spectrogram (torch.Tensor): Input tensor representing the spectrogram.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The model's output (logit for binary classification).\n",
        "        \"\"\"\n",
        "        return self.efficientnet(spectrogram)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9ydySeiGchks"
      },
      "outputs": [],
      "source": [
        "class ModelHandler:\n",
        "    \"\"\"Handles the model training, evaluation, and inference pipeline.\n",
        "\n",
        "    Attributes:\n",
        "        device (torch.device): The device on which the model is executed (e.g., 'cpu' or 'cuda').\n",
        "        model_path: Path to where .pth models should be saved.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 model_path: str,\n",
        "                 optimizer: torch.optim.Optimizer,\n",
        "                 loss_function: nn.Module,\n",
        "                 steps_per_decay = 5,\n",
        "                 lr_decay = 0.1):\n",
        "        \"\"\"Initializes the ModelHandler.\n",
        "\n",
        "        Args:\n",
        "            model_path (str | None): Path to the pre-trained model file (if available).\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model_path = model_path\n",
        "        self.optimizer = optimizer\n",
        "        self.lr_scheduler = opt.lr_scheduler.StepLR(self.optimizer, step_size=steps_per_decay, gamma=lr_decay)\n",
        "        self.loss_function = loss_function\n",
        "\n",
        "    def train_step(self, dataloader):\n",
        "        \"\"\"Trains the model for a single epoch.\n",
        "\n",
        "        Args:\n",
        "            dataloader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        avg_loss, acc = 0, 0\n",
        "        for in_tensor, labels in dataloader:\n",
        "            in_tensor, labels = in_tensor.to(self.device), labels.to(self.device)\n",
        "            labels = labels.float().unsqueeze(1)  # Ensure correct shape for BCE loss\n",
        "\n",
        "            logits = self.model(in_tensor) # Feed input into model\n",
        "\n",
        "            loss = self.loss_function(logits, labels)  # Calculate loss\n",
        "            avg_loss += loss.item()  # Add to cumulative loss\n",
        "\n",
        "            # Gradient descent\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Calculate batch accuracy and add it to cumulative accuracy\n",
        "            prediction_classes = torch.round(torch.sigmoid(logits))\n",
        "            batch_acc = torch.mean((prediction_classes == labels).float()).item()\n",
        "            acc += batch_acc\n",
        "\n",
        "        avg_loss /= len(dataloader)  # Calculate avg loss for epoch from cumulative loss\n",
        "        acc /= len(dataloader)  # Calculate avg accuracy for epoch from cumulative accuracy\n",
        "        train_results = {\"avg_loss_per_batch\": avg_loss, \"avg_acc_per_batch\": acc * 100}\n",
        "        return train_results\n",
        "\n",
        "    def val_step(self, dataloader):\n",
        "        \"\"\"Evaluates the model on the validation dataset.\n",
        "\n",
        "        Args:\n",
        "            dataloader (torch.utils.data.DataLoader): DataLoader for the validation dataset.\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.inference_mode():\n",
        "            avg_loss, acc = 0, 0\n",
        "            for in_tensor, labels in dataloader:\n",
        "                in_tensor, labels = in_tensor.to(self.device), labels.to(self.device)\n",
        "                labels = labels.float().unsqueeze(1)  # Ensure correct shape for BCE loss\n",
        "\n",
        "                logits = self.model(in_tensor)  # Feed input into model\n",
        "\n",
        "                loss = self.loss_function(logits, labels)  # Calculate loss\n",
        "                avg_loss += loss.item()  # Add to cumulative loss\n",
        "\n",
        "                # Calculate batch accuracy and add it to cumulative accuracy\n",
        "                prediction_classes = torch.round(torch.sigmoid(logits))\n",
        "                batch_acc = torch.mean((prediction_classes == labels).float()).item()\n",
        "                acc += batch_acc\n",
        "\n",
        "            avg_loss /= len(dataloader)  # Calculate avg loss for each epoch from cumulative loss\n",
        "            acc /= len(dataloader)  # Calculate avg accuracy for each epoch from cumulative accuracy\n",
        "            valid_results = {\"avg_loss_per_batch\": avg_loss, \"avg_acc_per_batch\": acc * 100}\n",
        "            return valid_results\n",
        "\n",
        "    def train(self, train_loader, epochs: int, model_name: str):\n",
        "        \"\"\"Trains the model\n",
        "\n",
        "        Args:\n",
        "            train_loader: DataLoader for the training datasets\n",
        "            epochs (int): Number of training epochs.\n",
        "            model_name (str): Name to save the trained model.\n",
        "        \"\"\"\n",
        "        self.model.to(self.device)\n",
        "        training_results = {\"epoch\": [], \"loss\": [], \"accuracy\": []}\n",
        "        validation_results = {\"epoch\": [], \"loss\": [], \"accuracy\": []}\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # Train the model\n",
        "            training_data = self.train_step(train_loader)\n",
        "            training_results[\"epoch\"].append(epoch)\n",
        "            training_results[\"loss\"].append(training_data[\"avg_loss_per_batch\"])\n",
        "            training_results[\"accuracy\"].append(training_data[\"avg_acc_per_batch\"])\n",
        "\n",
        "            # Check the validation loss after training\n",
        "            validation_data = self.val_step(val_loader)\n",
        "            validation_results[\"epoch\"].append(epoch)\n",
        "            validation_results[\"loss\"].append(validation_data[\"avg_loss_per_batch\"])\n",
        "            validation_results[\"accuracy\"].append(validation_data[\"avg_acc_per_batch\"])\n",
        "\n",
        "            # Adjust learning rate if necessary\n",
        "            if self.lr_scheduler:\n",
        "                self.lr_scheduler.step()\n",
        "\n",
        "            if epoch % 1 == 0:\n",
        "                print(f\"{epoch}:\")\n",
        "                print(f\"LR: {self.optimizer.param_groups[0]['lr']}\")\n",
        "                print(f\"Loss - {training_data['avg_loss_per_batch']:.5f} | Accuracy - {training_data['avg_acc_per_batch']:.2f}%\")\n",
        "                print(f\"VLoss - {validation_data['avg_loss_per_batch']:.5f} | VAccuracy - {validation_data['avg_acc_per_batch']:.2f}%\\n\")\n",
        "\n",
        "        self.save_model(model_state_dict=self.model.state_dict(), model_name=model_name)\n",
        "        return training_results, validation_results\n",
        "\n",
        "\n",
        "    def validate(self, val_loader, hyperparams: dict, save_best: bool = True) -> tuple[float, float]:\n",
        "        \"\"\"Validates the model on the validation dataset.\n",
        "\n",
        "        Args:\n",
        "            val_loader: DataLoader for the validation dataset.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (validation accuracy, validation loss)\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        val_losses_epoch, batch_sizes, accs = [], [], []\n",
        "        best_acc = -1\n",
        "        best_model_state = None  # Track the best model weights\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X_val, y_val in val_loader:\n",
        "                X_val = X_val.to(self.device)\n",
        "                y_val = y_val.to(self.device).float().unsqueeze(1)\n",
        "\n",
        "                y_prediction_val = self.model(X_val)  # forward pass\n",
        "                loss = self.loss_function(y_prediction_val, y_val)\n",
        "                val_losses_epoch.append(loss.item())\n",
        "\n",
        "                # Compute accuracy\n",
        "                y_prediction_val = torch.sigmoid(y_prediction_val)  # Convert logits to probabilities\n",
        "                prediction_classes = (y_prediction_val > 0.5).float()  # Convert to binary 0/1\n",
        "\n",
        "                acc = torch.mean((prediction_classes == y_val).float()).item()\n",
        "                accs.append(acc)\n",
        "                batch_sizes.append(X_val.shape[0])\n",
        "\n",
        "        # Compute final validation loss and accuracy\n",
        "        val_loss = np.mean(val_losses_epoch)\n",
        "        val_acc = np.average(accs, weights=batch_sizes)  # Weighted average accuracy\n",
        "\n",
        "        print(f'Validation accuracy: {val_acc*100:.2f}% | Validation loss: {val_loss:.4f}')\n",
        "\n",
        "        if save_best and val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_model_state = self.model.state_dict()\n",
        "\n",
        "            # Create model filename using hyperparameters\n",
        "            hyperparam_str = \"_\".join(f\"{key}:{value}\" for key, value in hyperparams.items())\n",
        "            model_filename = f\"model_{hyperparam_str}_{time.time()}.pth\"\n",
        "\n",
        "            # Save the best model\n",
        "            save_path = os.path.join(self.model_path, model_filename)\n",
        "            torch.save(best_model_state, save_path)\n",
        "            print(f\"Best model saved at: {save_path}\")\n",
        "        return val_acc, val_loss\n",
        "\n",
        "\n",
        "    def evaluate(self, test_loader) -> float:\n",
        "        \"\"\"Evaluates the model on the test dataset.\n",
        "\n",
        "        Args:\n",
        "            test_loader: DataLoader for the test dataset.\n",
        "        \"\"\"\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        batch_sizes, accs = [], []\n",
        "        with torch.no_grad():\n",
        "            for X_test, y_test, in test_loader:\n",
        "                X_test = X_test.to(self.device)\n",
        "                y_test = y_test.to(self.device)\n",
        "\n",
        "                prediction = self.model(X_test)\n",
        "                batch_sizes.append(X_test.shape[0])\n",
        "\n",
        "                prediction = torch.sigmoid(prediction)\n",
        "                prediction_classes = (prediction > 0.5).float() # This converts to binary classes 0 and 1\n",
        "\n",
        "                acc = torch.mean((prediction_classes == y_test).float()).item()\n",
        "                accs.append(acc)\n",
        "\n",
        "        # Return average accuracy\n",
        "        return 0.0 if not accs else np.average(accs, weights=batch_sizes)\n",
        "\n",
        "\n",
        "    def predict(self, spectrogram: torch.Tensor, model_name: str) -> int:\n",
        "        \"\"\"Performs inference on a single spectrogram.\n",
        "\n",
        "        Args:\n",
        "            spectrogram (torch.Tensor): Input spectrogram for inference.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The predicted output from the model.\n",
        "        \"\"\"\n",
        "        self.load_model(self.model_path +f\"/{model_name}\")\n",
        "        spectrogram = spectrogram.unsqueeze(0).to(self.device)\n",
        "\n",
        "        with torch.no_grad:\n",
        "            logits = self.model(spectrogram)\n",
        "\n",
        "            probability = torch.sigmoid(logits)\n",
        "\n",
        "            prediction = (probability > 0.5).float() # Turn probability into binary classificaiton\n",
        "\n",
        "        return prediction.item()\n",
        "\n",
        "\n",
        "    def save_model(self, model_state_dict: collections.OrderedDict, model_name: str | None) -> None:\n",
        "        \"\"\"Saves the model to the specified file path.\n",
        "\n",
        "        Args:\n",
        "            path (str): Path to save the model file.\n",
        "        \"\"\"\n",
        "        path = self.model_path + \"/\" + model_name\n",
        "        torch.save(model_state_dict, path)\n",
        "\n",
        "\n",
        "    def load_model(self, path: str) -> None:\n",
        "        \"\"\"Loads a model from the specified file path.\n",
        "\n",
        "        Args:\n",
        "            path (str): Path to the model file.\n",
        "        \"\"\"\n",
        "        self.model.load_state_dict(torch.load(path))\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TtrsfN7QcjgM"
      },
      "outputs": [],
      "source": [
        "class DataPipeline:\n",
        "    \"\"\"Processes datasets, including loading, splitting, and preparing for inference.\n",
        "\n",
        "    This class provides methods for loading datasets, processing them for training,\n",
        "    and preparing single instances for inference.\n",
        "\n",
        "    Attributes:\n",
        "        test_size (float): Proportion of the dataset to include in the test split.\n",
        "        val_size (float): Proportion of the dataset to include for validation.\n",
        "        audio_processor: AudioProcessor instance for handling audio processing.\n",
        "        image_processor: ImageProcessor instance for handling spectrogram or extracted features processing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, test_size: float, val_size: float):\n",
        "        \"\"\"Initializes the DatasetProcessor.\n",
        "\n",
        "        Args:\n",
        "            data_path (str): Path to the dataset file.\n",
        "            test_size (float): Proportion of the dataset to include in the test split.\n",
        "            audio_processor (AudioProcessor): Instance for handling audio processing.\n",
        "            image_processor (ImageProcessor): Instance for handling spectrogram processing.\n",
        "        \"\"\"\n",
        "        self.test_size = test_size\n",
        "        self.val_size = val_size\n",
        "\n",
        "    def load_dataset(self) -> TensorDataset:\n",
        "        \"\"\"Loads the dataset from the specified file path into a DataFrame.\"\"\"\n",
        "        tensors = []\n",
        "        labels = []\n",
        "\n",
        "        for label_folder, label_value in zip([\"positive\", \"negative\"], [1, 0]):\n",
        "            spectrogram_folder = '/content/gdrive/MyDrive/RespiraCheck/Cough Data/spectrograms'\n",
        "            output_dir = os.path.join(spectrogram_folder, label_folder)\n",
        "\n",
        "            for image_name in tqdm(os.listdir(output_dir)):\n",
        "                image_path = os.path.join(output_dir, image_name)\n",
        "                image_tensor = self.image_to_tensor(image_path)\n",
        "\n",
        "                tensors.append(image_tensor)\n",
        "                labels.append(label_value)\n",
        "\n",
        "        # Tensor of all features (N x D) - N is number of samples (377), D is feature dimension (3,224,224)\n",
        "        X = torch.stack(tensors)\n",
        "        # Tensor of all labels (N x 1) - 377x1\n",
        "        y = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "        return TensorDataset(X, y)\n",
        "\n",
        "\n",
        "    def image_to_tensor(self, image_path: str) -> torch.Tensor:\n",
        "        \"\"\"Converts a spectrogram image to a PyTorch tensor.\n",
        "\n",
        "        Args:\n",
        "            image_path (str): Path to the spectrogram image file.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The PyTorch tensor representation of the image.\n",
        "        \"\"\"\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),  # Resize to ResNet18 input size\n",
        "            transforms.ToTensor(),  # Convert image to tensor\n",
        "        ])\n",
        "\n",
        "        image = Image.open(image_path).convert(\"RGB\") # Convert from RGBA to RGB\n",
        "        tensor_image = transform(image)\n",
        "\n",
        "        return tensor_image  # shape will be 3, 224, 224\n",
        "\n",
        "    def create_dataloaders(self, batch_size, dataset_path = None, upsample = True) -> tuple[DataLoader, DataLoader, DataLoader]:\n",
        "        \"\"\"Splits the dataset into training and test sets.\n",
        "\n",
        "        Args:\n",
        "            batch_size (int): The batch size for the DataLoader.\n",
        "            dataset_path (str | None): Path to the TensorDataset file.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (train_df, test_df) - The training and testing DataFrames.\n",
        "        \"\"\"\n",
        "        if dataset_path:\n",
        "            print(f\"Loading dataset from {dataset_path}\")\n",
        "            dataset = torch.load(dataset_path, weights_only=False)\n",
        "        else:\n",
        "            print(\"Processing and loading dataset\")\n",
        "            dataset = self.load_dataset()\n",
        "\n",
        "        # Calculate sizes\n",
        "        test_size = round(self.test_size * len(dataset))\n",
        "        val_size = round(self.val_size * len(dataset))\n",
        "        train_size = round(len(dataset) - test_size - val_size)  # Remaining for training\n",
        "\n",
        "        # Perform split\n",
        "        train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "        # Upsample positive class\n",
        "        if upsample:\n",
        "            print(\"Upsampling data\")\n",
        "            labels = [label.item() for _, label in train_dataset]\n",
        "            train_counts = {}\n",
        "            for label in labels:\n",
        "                train_counts[label] = train_counts.get(label, 0) + 1\n",
        "            # print(train_counts)\n",
        "\n",
        "            weights = torch.where(torch.tensor(labels) == 0, 1 / train_counts[0], 1 / train_counts[1])\n",
        "            # print(labels[:5], weights[:5])\n",
        "\n",
        "            wr_sampler = WeightedRandomSampler(weights, int(len(train_dataset) * 1.5))\n",
        "\n",
        "            train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=wr_sampler)\n",
        "\n",
        "        else:\n",
        "            print(\"No upsampling\")\n",
        "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Create DataLoaders\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Count labels in train_loader\n",
        "        train_counts = {}\n",
        "        for _, labels in train_loader:\n",
        "            for label in labels:\n",
        "                train_counts[label.item()] = train_counts.get(label.item(), 0) + 1\n",
        "\n",
        "        # print(train_counts)\n",
        "\n",
        "        # Reduce memory footprint\n",
        "        dataset, train_dataset, val_dataset, test_dataset = None, None, None, None\n",
        "\n",
        "        return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "k3JgY0lnclQH"
      },
      "outputs": [],
      "source": [
        "import torch.optim as opt\n",
        "\n",
        "# Static hyperparameters\n",
        "EPOCHS = 20\n",
        "\n",
        "# Learning rate scheduler\n",
        "STEPS_PER_LR_DECAY = 20\n",
        "LR_DECAY = 0.5\n",
        "\n",
        "# Model parameters\n",
        "DROPOUT = 0.5\n",
        "\n",
        "# Training\n",
        "LOSS_FN = nn.BCEWithLogitsLoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgLkS_upcm6n",
        "outputId": "f5eb6d2c-5824-4160-f2c8-1815128244b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 104MB/s]\n"
          ]
        }
      ],
      "source": [
        "model = CNNModel(DROPOUT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItAMI1yPcohx",
        "outputId": "bd6c4472-f866-4615-cad4-50fddb0b0c21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing and loading dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1409/1409 [00:51<00:00, 27.20it/s]\n",
            "100%|██████████| 4274/4274 [02:38<00:00, 26.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upsampling data\n"
          ]
        }
      ],
      "source": [
        "datapipeline = DataPipeline(test_size=0.15, val_size=0.15)\n",
        "train_loader, val_loader, test_loader = datapipeline.create_dataloaders(batch_size=8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXMzHuqedyuC",
        "outputId": "526db7b9-fe11-4c16-9459-86ce60381ac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Training with batch size: 16, learning rate: 0.0002\n",
            "Processing and loading dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1409/1409 [00:29<00:00, 47.65it/s]\n",
            "100%|██████████| 4274/4274 [01:46<00:00, 40.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upsampling data\n",
            "\n",
            "📊 Dataset Split:\n",
            "- Training Samples: 3979\n",
            "- Validation Samples: 852\n",
            "- Test Samples: 852\n",
            "\n",
            "🔄 Epoch 1/20\n",
            "0:\n",
            "LR: 0.0002\n",
            "Loss - 0.69308 | Accuracy - 51.96%\n",
            "VLoss - 0.67317 | VAccuracy - 62.15%\n",
            "\n",
            "Validation accuracy: 62.68% | Validation loss: 0.6732\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/Cough Data/model_batch_size:16_lr:0.0002_1742504227.6740189.pth\n",
            "✅ New best validation loss: 0.6732 | Validation accuracy: 62.68%\n",
            "\n",
            "🔄 Epoch 2/20\n",
            "0:\n",
            "LR: 0.00019876883405951377\n",
            "Loss - 0.67740 | Accuracy - 57.00%\n",
            "VLoss - 0.67852 | VAccuracy - 56.25%\n",
            "\n",
            "Validation accuracy: 56.34% | Validation loss: 0.6785\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/Cough Data/model_batch_size:16_lr:0.0002_1742504259.0965977.pth\n",
            "🔄 No improvement in validation loss for 1 epochs\n",
            "\n",
            "🔄 Epoch 3/20\n",
            "0:\n",
            "LR: 0.00019510565162951537\n",
            "Loss - 0.67556 | Accuracy - 58.16%\n",
            "VLoss - 0.68158 | VAccuracy - 54.51%\n",
            "\n",
            "Validation accuracy: 54.58% | Validation loss: 0.6816\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/Cough Data/model_batch_size:16_lr:0.0002_1742504291.089956.pth\n",
            "🔄 No improvement in validation loss for 2 epochs\n",
            "\n",
            "🔄 Epoch 4/20\n",
            "0:\n",
            "LR: 0.0001891006524188368\n",
            "Loss - 0.66476 | Accuracy - 59.89%\n",
            "VLoss - 0.65780 | VAccuracy - 59.72%\n",
            "\n",
            "Validation accuracy: 59.86% | Validation loss: 0.6578\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/Cough Data/model_batch_size:16_lr:0.0002_1742504323.6767328.pth\n",
            "✅ New best validation loss: 0.6578 | Validation accuracy: 59.86%\n",
            "\n",
            "🔄 Epoch 5/20\n",
            "0:\n",
            "LR: 0.00018090169943749476\n",
            "Loss - 0.65378 | Accuracy - 61.58%\n",
            "VLoss - 0.64965 | VAccuracy - 63.66%\n",
            "\n",
            "Validation accuracy: 63.85% | Validation loss: 0.6496\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/Cough Data/model_batch_size:16_lr:0.0002_1742504356.6789777.pth\n",
            "✅ New best validation loss: 0.6496 | Validation accuracy: 63.85%\n",
            "\n",
            "🔄 Epoch 6/20\n",
            "0:\n",
            "LR: 0.00017071067811865476\n",
            "Loss - 0.64502 | Accuracy - 62.60%\n",
            "VLoss - 0.64497 | VAccuracy - 61.57%\n",
            "\n",
            "Validation accuracy: 61.74% | Validation loss: 0.6450\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/Cough Data/model_batch_size:16_lr:0.0002_1742504389.6528554.pth\n",
            "✅ New best validation loss: 0.6450 | Validation accuracy: 61.74%\n",
            "\n",
            "🔄 Epoch 7/20\n",
            "0:\n",
            "LR: 0.00015877852522924732\n",
            "Loss - 0.64069 | Accuracy - 62.95%\n",
            "VLoss - 0.64176 | VAccuracy - 61.23%\n",
            "\n",
            "Validation accuracy: 61.38% | Validation loss: 0.6418\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/Cough Data/model_batch_size:16_lr:0.0002_1742504422.3510606.pth\n",
            "✅ New best validation loss: 0.6418 | Validation accuracy: 61.38%\n",
            "\n",
            "🔄 Epoch 8/20\n",
            "0:\n",
            "LR: 0.00014539904997395468\n",
            "Loss - 0.62814 | Accuracy - 65.35%\n",
            "VLoss - 0.63482 | VAccuracy - 63.31%\n",
            "\n",
            "Validation accuracy: 63.50% | Validation loss: 0.6348\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/Cough Data/model_batch_size:16_lr:0.0002_1742504455.1132455.pth\n",
            "✅ New best validation loss: 0.6348 | Validation accuracy: 63.50%\n",
            "\n",
            "🔄 Epoch 9/20\n",
            "0:\n",
            "LR: 0.00013090169943749476\n",
            "Loss - 0.61991 | Accuracy - 65.95%\n",
            "VLoss - 0.63984 | VAccuracy - 62.04%\n",
            "\n",
            "Validation accuracy: 62.21% | Validation loss: 0.6398\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/Cough Data/model_batch_size:16_lr:0.0002_1742504487.666885.pth\n",
            "🔄 No improvement in validation loss for 1 epochs\n",
            "\n",
            "🔄 Epoch 10/20\n",
            "0:\n",
            "LR: 0.00011564344650402312\n",
            "Loss - 0.61532 | Accuracy - 66.97%\n",
            "VLoss - 0.64728 | VAccuracy - 60.76%\n",
            "\n",
            "Validation accuracy: 60.92% | Validation loss: 0.6473\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/Cough Data/model_batch_size:16_lr:0.0002_1742504520.232022.pth\n",
            "🔄 No improvement in validation loss for 2 epochs\n",
            "\n",
            "🔄 Epoch 11/20\n",
            "0:\n",
            "LR: 0.00010000000000000002\n",
            "Loss - 0.60427 | Accuracy - 67.93%\n",
            "VLoss - 0.62552 | VAccuracy - 64.47%\n",
            "\n",
            "Validation accuracy: 64.67% | Validation loss: 0.6255\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/Cough Data/model_batch_size:16_lr:0.0002_1742504552.7373111.pth\n",
            "✅ New best validation loss: 0.6255 | Validation accuracy: 64.67%\n",
            "\n",
            "🔄 Epoch 12/20\n",
            "0:\n",
            "LR: 8.435655349597696e-05\n",
            "Loss - 0.59980 | Accuracy - 68.16%\n",
            "VLoss - 0.63078 | VAccuracy - 63.31%\n",
            "\n",
            "Validation accuracy: 63.50% | Validation loss: 0.6308\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/Cough Data/model_batch_size:16_lr:0.0002_1742504585.3970962.pth\n",
            "🔄 No improvement in validation loss for 1 epochs\n",
            "\n",
            "🔄 Epoch 13/20\n",
            "0:\n",
            "LR: 6.909830056250529e-05\n",
            "Loss - 0.59292 | Accuracy - 68.36%\n",
            "VLoss - 0.62116 | VAccuracy - 65.05%\n",
            "\n",
            "Validation accuracy: 65.26% | Validation loss: 0.6212\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/Cough Data/model_batch_size:16_lr:0.0002_1742504618.0450432.pth\n",
            "✅ New best validation loss: 0.6212 | Validation accuracy: 65.26%\n",
            "\n",
            "🔄 Epoch 14/20\n",
            "0:\n",
            "LR: 5.460095002604534e-05\n",
            "Loss - 0.59249 | Accuracy - 69.77%\n",
            "VLoss - 0.62145 | VAccuracy - 64.93%\n",
            "\n",
            "Validation accuracy: 65.14% | Validation loss: 0.6215\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/Cough Data/model_batch_size:16_lr:0.0002_1742504650.583363.pth\n",
            "🔄 No improvement in validation loss for 1 epochs\n",
            "\n",
            "🔄 Epoch 15/20\n",
            "0:\n",
            "LR: 4.122147477075271e-05\n",
            "Loss - 0.58179 | Accuracy - 69.54%\n",
            "VLoss - 0.62141 | VAccuracy - 63.66%\n",
            "\n",
            "Validation accuracy: 63.85% | Validation loss: 0.6214\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/Cough Data/model_batch_size:16_lr:0.0002_1742504683.2895422.pth\n",
            "🔄 No improvement in validation loss for 2 epochs\n",
            "\n",
            "🔄 Epoch 16/20\n",
            "0:\n",
            "LR: 2.928932188134526e-05\n",
            "Loss - 0.58568 | Accuracy - 69.02%\n",
            "VLoss - 0.60705 | VAccuracy - 66.55%\n",
            "\n",
            "Validation accuracy: 66.78% | Validation loss: 0.6071\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/Cough Data/model_batch_size:16_lr:0.0002_1742504715.799859.pth\n",
            "✅ New best validation loss: 0.6071 | Validation accuracy: 66.78%\n",
            "\n",
            "🔄 Epoch 17/20\n",
            "0:\n",
            "LR: 1.909830056250527e-05\n",
            "Loss - 0.57651 | Accuracy - 69.86%\n",
            "VLoss - 0.62335 | VAccuracy - 65.05%\n",
            "\n",
            "Validation accuracy: 65.26% | Validation loss: 0.6234\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/Cough Data/model_batch_size:16_lr:0.0002_1742504748.4582925.pth\n",
            "🔄 No improvement in validation loss for 1 epochs\n",
            "\n",
            "🔄 Epoch 18/20\n",
            "0:\n",
            "LR: 1.0899347581163223e-05\n",
            "Loss - 0.56539 | Accuracy - 71.30%\n",
            "VLoss - 0.62403 | VAccuracy - 64.12%\n",
            "\n",
            "Validation accuracy: 64.32% | Validation loss: 0.6240\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/Cough Data/model_batch_size:16_lr:0.0002_1742504781.0169234.pth\n",
            "🔄 No improvement in validation loss for 2 epochs\n",
            "\n",
            "🔄 Epoch 19/20\n",
            "0:\n",
            "LR: 4.8943483704846475e-06\n",
            "Loss - 0.57275 | Accuracy - 70.69%\n",
            "VLoss - 0.61317 | VAccuracy - 65.74%\n",
            "\n",
            "Validation accuracy: 65.96% | Validation loss: 0.6132\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/Cough Data/model_batch_size:16_lr:0.0002_1742504813.6512063.pth\n",
            "🔄 No improvement in validation loss for 3 epochs\n",
            "\n",
            "🔄 Epoch 20/20\n",
            "0:\n",
            "LR: 6.155829702431171e-07\n",
            "Loss - 0.57414 | Accuracy - 70.02%\n",
            "VLoss - 0.61207 | VAccuracy - 65.62%\n",
            "\n",
            "Validation accuracy: 65.85% | Validation loss: 0.6121\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/Cough Data/model_batch_size:16_lr:0.0002_1742504846.2722151.pth\n",
            "🔄 No improvement in validation loss for 4 epochs\n",
            "\n",
            "🎯 Test accuracy: 60.84% 🚀 Best model saved!\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as opt\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torchaudio.transforms as T\n",
        "import torch.nn.utils\n",
        "import numpy as np\n",
        "\n",
        "# Define fixed hyperparameters\n",
        "batch_size = 16  # Reduced batch size for better generalization\n",
        "learning_rate = 0.0002  # More stable learning rate\n",
        "weight_decay = 5e-4  # Regularization strength\n",
        "dropout_rate = 0.6  # Increased dropout to reduce overfitting\n",
        "patience = 15  # Increased patience for early stopping\n",
        "\n",
        "print(f\"\\n🚀 Training with batch size: {batch_size}, learning rate: {learning_rate}\")\n",
        "\n",
        "# Initialize model (Use custom CNNModel with EfficientNet-B0 backbone)\n",
        "cnn_model = CNNModel(dropout=dropout_rate)\n",
        "\n",
        "# Choose optimizer (SGD with momentum for better generalization)\n",
        "optimizer = opt.SGD(params=cnn_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
        "\n",
        "# Learning rate scheduler (CosineAnnealingLR for smooth LR decay)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=20)\n",
        "\n",
        "# Data Augmentation (SpecAugment with additional Mixup)\n",
        "def augment_spectrogram(spectrogram):\n",
        "    spectrogram = T.FrequencyMasking(freq_mask_param=15)(spectrogram)\n",
        "    spectrogram = T.TimeMasking(time_mask_param=25)(spectrogram)\n",
        "    spectrogram = T.Vol(0.8)(spectrogram)  # Random volume adjustment\n",
        "    return spectrogram\n",
        "\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    index = torch.randperm(x.size(0))\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "# Create ModelHandler\n",
        "model_handler = ModelHandler(model=cnn_model,\n",
        "                             model_path=\"/content/gdrive/MyDrive/RespiraCheck/Cough Data\",\n",
        "                             optimizer=optimizer,\n",
        "                             loss_function=LOSS_FN,\n",
        "                             steps_per_decay=STEPS_PER_LR_DECAY,\n",
        "                             lr_decay=LR_DECAY)\n",
        "\n",
        "# Load dataset with the chosen batch size\n",
        "datapipeline = DataPipeline(test_size=0.15, val_size=0.15)\n",
        "train_loader, val_loader, test_loader = datapipeline.create_dataloaders(batch_size=batch_size, upsample=True)\n",
        "\n",
        "# Debug: Check dataset distribution\n",
        "print(\"\\n📊 Dataset Split:\")\n",
        "print(f\"- Training Samples: {len(train_loader.dataset)}\")\n",
        "print(f\"- Validation Samples: {len(val_loader.dataset)}\")\n",
        "print(f\"- Test Samples: {len(test_loader.dataset)}\")\n",
        "\n",
        "# Training loop with early stopping\n",
        "best_val_loss = float(\"inf\")\n",
        "best_model = None\n",
        "best_acc = 0.0\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n🔄 Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    # Train\n",
        "    train_results, val_results = model_handler.train(train_loader=train_loader, epochs=1, model_name=\"CNN_EfficientNet\")\n",
        "    train_loss = train_results[\"loss\"][-1]  # Get the last recorded loss value\n",
        "\n",
        "    # Validate\n",
        "    val_acc, val_loss = model_handler.validate(val_loader, {\"batch_size\": batch_size, \"lr\": learning_rate})\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step()\n",
        "\n",
        "    # Gradient Clipping to stabilize training\n",
        "    torch.nn.utils.clip_grad_norm_(cnn_model.parameters(), max_norm=1.0)\n",
        "\n",
        "    # Check if validation loss improved\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_acc = val_acc\n",
        "        best_model = model_handler\n",
        "        epochs_without_improvement = 0  # Reset counter if there's improvement\n",
        "        print(f\"✅ New best validation loss: {best_val_loss:.4f} | Validation accuracy: {val_acc*100:.2f}%\")\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        print(f\"🔄 No improvement in validation loss for {epochs_without_improvement} epochs\")\n",
        "\n",
        "    # Early Stopping: Stop training if there's no improvement for `patience` epochs\n",
        "    if epochs_without_improvement >= patience:\n",
        "        print(f\"⏹️ Early stopping triggered due to no improvement in validation loss.\")\n",
        "        break\n",
        "\n",
        "# Final Testing\n",
        "if best_model:\n",
        "    test_acc = best_model.evaluate(test_loader)\n",
        "    print(f\"\\n🎯 Test accuracy: {test_acc*100:.2f}% 🚀 Best model saved!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwlwx85BhN3s",
        "outputId": "1cf1086c-5f80-48e4-ea8a-26bd7138d738"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🚀 Training with batch size: 16, learning rate: 0.0002\n",
            "Processing and loading dataset\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1409/1409 [00:32<00:00, 44.03it/s]\n",
            "100%|██████████| 4274/4274 [01:46<00:00, 40.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upsampling data\n",
            "\n",
            "📊 Dataset Split:\n",
            "- Training Samples: 3979\n",
            "- Validation Samples: 852\n",
            "- Test Samples: 852\n",
            "\n",
            "🔄 Epoch 1/20\n",
            "0:\n",
            "LR: 0.0002\n",
            "Loss - 0.69127 | Accuracy - 51.96%\n",
            "VLoss - 0.66768 | VAccuracy - 62.96%\n",
            "\n",
            "📊 Validation Accuracy: 74.65% | F1-score: 0.6381\n",
            "✅ New best F1-score: 0.6381\n",
            "\n",
            "🔄 Epoch 2/20\n",
            "0:\n",
            "LR: 0.00019876883405951377\n",
            "Loss - 0.67607 | Accuracy - 58.23%\n",
            "VLoss - 0.64925 | VAccuracy - 66.20%\n",
            "\n",
            "📊 Validation Accuracy: 74.65% | F1-score: 0.6381\n",
            "🔄 No improvement in F1-score for 1 epochs\n",
            "\n",
            "🔄 Epoch 3/20\n",
            "0:\n",
            "LR: 0.00019510565162951537\n",
            "Loss - 0.66663 | Accuracy - 60.54%\n",
            "VLoss - 0.62979 | VAccuracy - 68.98%\n",
            "\n",
            "📊 Validation Accuracy: 74.65% | F1-score: 0.6381\n",
            "🔄 No improvement in F1-score for 2 epochs\n",
            "\n",
            "🔄 Epoch 4/20\n",
            "0:\n",
            "LR: 0.0001891006524188368\n",
            "Loss - 0.65940 | Accuracy - 61.09%\n",
            "VLoss - 0.63138 | VAccuracy - 66.32%\n",
            "\n",
            "📊 Validation Accuracy: 74.65% | F1-score: 0.6381\n",
            "🔄 No improvement in F1-score for 3 epochs\n",
            "\n",
            "🔄 Epoch 5/20\n",
            "0:\n",
            "LR: 0.00018090169943749476\n",
            "Loss - 0.65346 | Accuracy - 61.38%\n",
            "VLoss - 0.61378 | VAccuracy - 67.94%\n",
            "\n",
            "📊 Validation Accuracy: 74.65% | F1-score: 0.6381\n",
            "🔄 No improvement in F1-score for 4 epochs\n",
            "\n",
            "🔄 Epoch 6/20\n",
            "0:\n",
            "LR: 0.00017071067811865476\n",
            "Loss - 0.63996 | Accuracy - 63.72%\n",
            "VLoss - 0.62079 | VAccuracy - 67.25%\n",
            "\n",
            "📊 Validation Accuracy: 74.65% | F1-score: 0.6381\n",
            "🔄 No improvement in F1-score for 5 epochs\n",
            "\n",
            "🔄 Epoch 7/20\n",
            "0:\n",
            "LR: 0.00015877852522924732\n",
            "Loss - 0.63350 | Accuracy - 63.76%\n",
            "VLoss - 0.61091 | VAccuracy - 67.48%\n",
            "\n",
            "📊 Validation Accuracy: 74.65% | F1-score: 0.6381\n",
            "🔄 No improvement in F1-score for 6 epochs\n",
            "\n",
            "🔄 Epoch 8/20\n",
            "0:\n",
            "LR: 0.00014539904997395468\n",
            "Loss - 0.62227 | Accuracy - 64.98%\n",
            "VLoss - 0.58797 | VAccuracy - 68.87%\n",
            "\n",
            "📊 Validation Accuracy: 74.65% | F1-score: 0.6381\n",
            "🔄 No improvement in F1-score for 7 epochs\n",
            "\n",
            "🔄 Epoch 9/20\n",
            "0:\n",
            "LR: 0.00013090169943749476\n",
            "Loss - 0.60566 | Accuracy - 67.85%\n",
            "VLoss - 0.59263 | VAccuracy - 67.48%\n",
            "\n",
            "📊 Validation Accuracy: 74.65% | F1-score: 0.6381\n",
            "🔄 No improvement in F1-score for 8 epochs\n",
            "\n",
            "🔄 Epoch 10/20\n",
            "0:\n",
            "LR: 0.00011564344650402312\n",
            "Loss - 0.60563 | Accuracy - 67.48%\n",
            "VLoss - 0.58023 | VAccuracy - 70.02%\n",
            "\n",
            "📊 Validation Accuracy: 74.65% | F1-score: 0.6381\n",
            "🔄 No improvement in F1-score for 9 epochs\n",
            "\n",
            "🔄 Epoch 11/20\n",
            "0:\n",
            "LR: 0.00010000000000000002\n",
            "Loss - 0.59708 | Accuracy - 69.00%\n",
            "VLoss - 0.60950 | VAccuracy - 66.78%\n",
            "\n",
            "📊 Validation Accuracy: 74.65% | F1-score: 0.6381\n",
            "🔄 No improvement in F1-score for 10 epochs\n",
            "\n",
            "🔄 Epoch 12/20\n",
            "0:\n",
            "LR: 8.435655349597696e-05\n",
            "Loss - 0.58262 | Accuracy - 70.43%\n",
            "VLoss - 0.59525 | VAccuracy - 68.17%\n",
            "\n",
            "📊 Validation Accuracy: 74.65% | F1-score: 0.6381\n",
            "🔄 No improvement in F1-score for 11 epochs\n",
            "\n",
            "🔄 Epoch 13/20\n",
            "0:\n",
            "LR: 6.909830056250529e-05\n",
            "Loss - 0.57596 | Accuracy - 70.76%\n",
            "VLoss - 0.59100 | VAccuracy - 68.75%\n",
            "\n",
            "📊 Validation Accuracy: 74.65% | F1-score: 0.6381\n",
            "🔄 No improvement in F1-score for 12 epochs\n",
            "\n",
            "🔄 Epoch 14/20\n",
            "0:\n",
            "LR: 5.460095002604534e-05\n",
            "Loss - 0.58196 | Accuracy - 70.02%\n",
            "VLoss - 0.61831 | VAccuracy - 65.51%\n",
            "\n",
            "📊 Validation Accuracy: 74.65% | F1-score: 0.6381\n",
            "🔄 No improvement in F1-score for 13 epochs\n",
            "\n",
            "🔄 Epoch 15/20\n",
            "0:\n",
            "LR: 4.122147477075271e-05\n",
            "Loss - 0.56951 | Accuracy - 70.91%\n",
            "VLoss - 0.58496 | VAccuracy - 68.75%\n",
            "\n",
            "📊 Validation Accuracy: 74.65% | F1-score: 0.6381\n",
            "🔄 No improvement in F1-score for 14 epochs\n",
            "\n",
            "🔄 Epoch 16/20\n",
            "0:\n",
            "LR: 2.928932188134526e-05\n",
            "Loss - 0.56331 | Accuracy - 71.43%\n",
            "VLoss - 0.58872 | VAccuracy - 68.17%\n",
            "\n",
            "📊 Validation Accuracy: 74.65% | F1-score: 0.6381\n",
            "🔄 No improvement in F1-score for 15 epochs\n",
            "⏹️ Early stopping triggered due to no improvement in F1-score.\n",
            "\n",
            "🎯 Test Accuracy: 72.42% | Test F1-score: 0.6083 🚀\n",
            "\n",
            "🎯 Test Accuracy: 72.42% | Test F1-score: 0.6083 🚀 Best model saved!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as opt\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torchaudio.transforms as T\n",
        "import torch.nn.utils\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "# Define fixed hyperparameters\n",
        "batch_size = 16  # Reduced batch size for better generalization\n",
        "learning_rate = 0.0002  # More stable learning rate\n",
        "weight_decay = 5e-4  # Regularization strength\n",
        "dropout_rate = 0.6  # Increased dropout to reduce overfitting\n",
        "patience = 15  # Early stopping patience\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\\n🚀 Training with batch size: {batch_size}, learning rate: {learning_rate}\")\n",
        "\n",
        "# Initialize model (Use custom CNNModel with EfficientNet-B0 backbone)\n",
        "cnn_model = CNNModel(dropout=dropout_rate).to(device)  # Move model to device\n",
        "\n",
        "# Choose optimizer (SGD with momentum for better generalization)\n",
        "optimizer = opt.SGD(params=cnn_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
        "\n",
        "# Learning rate scheduler (CosineAnnealingLR for smooth LR decay)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=20)\n",
        "\n",
        "# Data Augmentation (SpecAugment + Mixup)\n",
        "def augment_spectrogram(spectrogram):\n",
        "    spectrogram = T.FrequencyMasking(freq_mask_param=15)(spectrogram)\n",
        "    spectrogram = T.TimeMasking(time_mask_param=25)(spectrogram)\n",
        "    spectrogram = T.Vol(0.8)(spectrogram)  # Random volume adjustment\n",
        "    return spectrogram\n",
        "\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    index = torch.randperm(x.size(0))\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "# Create ModelHandler\n",
        "model_handler = ModelHandler(\n",
        "    model=cnn_model,\n",
        "    model_path=\"/content/gdrive/MyDrive/RespiraCheck/Cough Data\",\n",
        "    optimizer=optimizer,\n",
        "    loss_function=LOSS_FN,\n",
        "    steps_per_decay=STEPS_PER_LR_DECAY,\n",
        "    lr_decay=LR_DECAY\n",
        ")\n",
        "\n",
        "# Load dataset with the chosen batch size\n",
        "datapipeline = DataPipeline(test_size=0.15, val_size=0.15)\n",
        "train_loader, val_loader, test_loader = datapipeline.create_dataloaders(batch_size=batch_size, upsample=True)\n",
        "\n",
        "# Debug: Check dataset distribution\n",
        "print(\"\\n📊 Dataset Split:\")\n",
        "print(f\"- Training Samples: {len(train_loader.dataset)}\")\n",
        "print(f\"- Validation Samples: {len(val_loader.dataset)}\")\n",
        "print(f\"- Test Samples: {len(test_loader.dataset)}\")\n",
        "\n",
        "# Function to Validate and Compute F1-score\n",
        "def validate(model_handler, val_loader):\n",
        "    model_handler.model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.to(device), targets.to(device)  # Move to device\n",
        "            outputs = model_handler.model(inputs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    val_acc = accuracy_score(all_targets, all_preds)\n",
        "    val_f1 = f1_score(all_targets, all_preds, average=\"weighted\")\n",
        "\n",
        "    return val_acc, val_f1\n",
        "\n",
        "# Training loop with early stopping\n",
        "best_val_loss = float(\"inf\")\n",
        "best_model = None\n",
        "best_acc = 0.0\n",
        "best_f1_score = 0.0\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n🔄 Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    # Train\n",
        "    train_results, val_results = model_handler.train(train_loader=train_loader, epochs=1, model_name=\"CNN_EfficientNet\")\n",
        "\n",
        "    # Validate and Compute F1-score\n",
        "    val_acc, val_f1 = validate(model_handler, val_loader)\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"📊 Validation Accuracy: {val_acc*100:.2f}% | F1-score: {val_f1:.4f}\")\n",
        "\n",
        "    # Check for improvement\n",
        "    if val_f1 > best_f1_score:\n",
        "        best_f1_score = val_f1\n",
        "        best_model = model_handler\n",
        "        epochs_without_improvement = 0  # Reset counter if there's improvement\n",
        "        print(f\"✅ New best F1-score: {best_f1_score:.4f}\")\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        print(f\"🔄 No improvement in F1-score for {epochs_without_improvement} epochs\")\n",
        "\n",
        "    # Early Stopping\n",
        "    if epochs_without_improvement >= patience:\n",
        "        print(f\"⏹️ Early stopping triggered due to no improvement in F1-score.\")\n",
        "        break\n",
        "\n",
        "# Function to Evaluate Model on Test Set\n",
        "def evaluate(model_handler, test_loader):\n",
        "    model_handler.model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.to(device), targets.to(device)  # Move to device\n",
        "            outputs = model_handler.model(inputs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    test_acc = accuracy_score(all_targets, all_preds)\n",
        "    test_f1 = f1_score(all_targets, all_preds, average=\"weighted\")\n",
        "\n",
        "    print(f\"\\n🎯 Test Accuracy: {test_acc*100:.2f}% | Test F1-score: {test_f1:.4f} 🚀\")\n",
        "    return test_acc, test_f1\n",
        "\n",
        "# Final Testing\n",
        "if best_model:\n",
        "    test_acc, test_f1 = evaluate(best_model, test_loader)\n",
        "    print(f\"\\n🎯 Test Accuracy: {test_acc*100:.2f}% | Test F1-score: {test_f1:.4f} 🚀 Best model saved!\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}