{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gYscyCg6b7rZ"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import collections\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import random_split, DataLoader, TensorDataset, WeightedRandomSampler\n",
        "from torchvision import transforms\n",
        "import io\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0AUU9kYcUAv",
        "outputId": "83c2bc6e-75d4-4799-ee64-1568acc92cdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "['Cough Data']\n",
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "print(os.listdir('/content/gdrive/MyDrive/RespiraCheck'))\n",
        "print(os.path.exists('/content/gdrive/MyDrive/RespiraCheck/Cough Data/spectrograms/positive'))\n",
        "print(os.path.exists('/content/gdrive/MyDrive/RespiraCheck/Cough Data/spectrograms/negative'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tbLR_B94cdPR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    \"\"\"A convolutional neural network model based on EfficientNet for spectrogram processing.\"\"\"\n",
        "\n",
        "    def __init__(self, dropout: float = 0.0):\n",
        "        \"\"\"Initializes the CNNModel using EfficientNet-B3 with an optional dropout layer.\n",
        "\n",
        "        Args:\n",
        "            dropout (float): Dropout probability before the final classification layer.\n",
        "        \"\"\"\n",
        "        super(CNNModel, self).__init__()\n",
        "\n",
        "        # Load EfficientNet-B3 with pre-trained weights\n",
        "        self.efficientnet = models.efficientnet_b3(weights='IMAGENET1K_V1')\n",
        "\n",
        "        # Get the number of features from the last layer of EfficientNet\n",
        "        num_features = self.efficientnet.classifier[1].in_features\n",
        "\n",
        "        # Replace the classifier with a new one for binary classification (2 outputs)\n",
        "        self.efficientnet.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),          # Dropout before final layer\n",
        "            nn.Linear(num_features, 2)      # Output 2 logits (for CrossEntropyLoss)\n",
        "        )\n",
        "\n",
        "        # Initialize the new FC layer weights\n",
        "        nn.init.normal_(self.efficientnet.classifier[1].weight, mean=0.0, std=0.01)\n",
        "        nn.init.zeros_(self.efficientnet.classifier[1].bias)\n",
        "\n",
        "    def forward(self, spectrogram: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Defines the forward pass for EfficientNet.\n",
        "\n",
        "        Args:\n",
        "            spectrogram (torch.Tensor): Input tensor representing the spectrogram.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output logits for both classes.\n",
        "        \"\"\"\n",
        "        return self.efficientnet(spectrogram)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9ydySeiGchks"
      },
      "outputs": [],
      "source": [
        "class ModelHandler:\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 model_path: str,\n",
        "                 optimizer: torch.optim.Optimizer,\n",
        "                 loss_function: nn.Module,\n",
        "                 steps_per_decay=5,\n",
        "                 lr_decay=0.1):\n",
        "        self.model = model\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model_path = model_path\n",
        "        self.optimizer = optimizer\n",
        "        self.lr_scheduler = opt.lr_scheduler.StepLR(self.optimizer, step_size=steps_per_decay, gamma=lr_decay)\n",
        "        self.loss_function = loss_function\n",
        "\n",
        "    def train_step(self, dataloader):\n",
        "        self.model.train()\n",
        "        avg_loss, acc = 0, 0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "\n",
        "            logits = self.model(inputs)\n",
        "            loss = self.loss_function(logits, labels)\n",
        "            avg_loss += loss.item()\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            acc += (preds == labels).float().mean().item()\n",
        "\n",
        "        avg_loss /= len(dataloader)\n",
        "        acc /= len(dataloader)\n",
        "        return {\"avg_loss_per_batch\": avg_loss, \"avg_acc_per_batch\": acc * 100}\n",
        "\n",
        "    def val_step(self, dataloader):\n",
        "        self.model.eval()\n",
        "        avg_loss, acc = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "\n",
        "                logits = self.model(inputs)\n",
        "                loss = self.loss_function(logits, labels)\n",
        "                avg_loss += loss.item()\n",
        "\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                acc += (preds == labels).float().mean().item()\n",
        "\n",
        "        avg_loss /= len(dataloader)\n",
        "        acc /= len(dataloader)\n",
        "        return {\"avg_loss_per_batch\": avg_loss, \"avg_acc_per_batch\": acc * 100}\n",
        "\n",
        "    def train(self, train_loader, val_loader, epochs: int, model_name: str):\n",
        "        self.model.to(self.device)\n",
        "        training_results = {\"epoch\": [], \"loss\": [], \"accuracy\": []}\n",
        "        validation_results = {\"epoch\": [], \"loss\": [], \"accuracy\": []}\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            training_data = self.train_step(train_loader)\n",
        "            validation_data = self.val_step(val_loader)\n",
        "\n",
        "            training_results[\"epoch\"].append(epoch)\n",
        "            training_results[\"loss\"].append(training_data[\"avg_loss_per_batch\"])\n",
        "            training_results[\"accuracy\"].append(training_data[\"avg_acc_per_batch\"])\n",
        "\n",
        "            validation_results[\"epoch\"].append(epoch)\n",
        "            validation_results[\"loss\"].append(validation_data[\"avg_loss_per_batch\"])\n",
        "            validation_results[\"accuracy\"].append(validation_data[\"avg_acc_per_batch\"])\n",
        "\n",
        "            if self.lr_scheduler:\n",
        "                self.lr_scheduler.step()\n",
        "\n",
        "            print(f\"{epoch}:\")\n",
        "            print(f\"LR: {self.optimizer.param_groups[0]['lr']}\")\n",
        "            print(f\"Loss - {training_data['avg_loss_per_batch']:.5f} | Accuracy - {training_data['avg_acc_per_batch']:.2f}%\")\n",
        "            print(f\"VLoss - {validation_data['avg_loss_per_batch']:.5f} | VAccuracy - {validation_data['avg_acc_per_batch']:.2f}%\\n\")\n",
        "\n",
        "        self.save_model(model_state_dict=self.model.state_dict(), model_name=model_name)\n",
        "        return training_results, validation_results\n",
        "\n",
        "    def evaluate(self, test_loader):\n",
        "        self.model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "\n",
        "                logits = self.model(inputs)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        acc = accuracy_score(all_labels, all_preds)\n",
        "        f1 = f1_score(all_labels, all_preds)\n",
        "        print(f\"Test Accuracy: {acc:.4f}, Test F1 Score: {f1:.4f}\")\n",
        "        return acc, f1\n",
        "\n",
        "    def predict(self, spectrogram: torch.Tensor, model_name: str) -> int:\n",
        "        self.load_model(self.model_path + f\"/{model_name}\")\n",
        "        spectrogram = spectrogram.unsqueeze(0).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(spectrogram)\n",
        "            prediction = torch.argmax(logits, dim=1)\n",
        "\n",
        "        return prediction.item()\n",
        "\n",
        "    def save_model(self, model_state_dict: collections.OrderedDict, model_name: str | None) -> None:\n",
        "        path = self.model_path + \"/\" + model_name\n",
        "        torch.save(model_state_dict, path)\n",
        "\n",
        "    def load_model(self, path: str) -> None:\n",
        "        self.model.load_state_dict(torch.load(path))\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TtrsfN7QcjgM"
      },
      "outputs": [],
      "source": [
        "class DataPipeline:\n",
        "    \"\"\"Processes datasets, including loading, splitting, and preparing for inference.\n",
        "\n",
        "    This class provides methods for loading datasets, processing them for training,\n",
        "    and preparing single instances for inference.\n",
        "\n",
        "    Attributes:\n",
        "        test_size (float): Proportion of the dataset to include in the test split.\n",
        "        val_size (float): Proportion of the dataset to include for validation.\n",
        "        audio_processor: AudioProcessor instance for handling audio processing.\n",
        "        image_processor: ImageProcessor instance for handling spectrogram or extracted features processing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, test_size: float, val_size: float):\n",
        "        \"\"\"Initializes the DatasetProcessor.\n",
        "\n",
        "        Args:\n",
        "            data_path (str): Path to the dataset file.\n",
        "            test_size (float): Proportion of the dataset to include in the test split.\n",
        "            audio_processor (AudioProcessor): Instance for handling audio processing.\n",
        "            image_processor (ImageProcessor): Instance for handling spectrogram processing.\n",
        "        \"\"\"\n",
        "        self.test_size = test_size\n",
        "        self.val_size = val_size\n",
        "\n",
        "    def load_dataset(self) -> TensorDataset:\n",
        "        \"\"\"Loads the dataset from the specified file path into a DataFrame.\"\"\"\n",
        "        tensors = []\n",
        "        labels = []\n",
        "\n",
        "        for label_folder, label_value in zip([\"positive\", \"negative\"], [1, 0]):\n",
        "            spectrogram_folder = '/content/gdrive/MyDrive/RespiraCheck/Cough Data/spectrograms'\n",
        "            output_dir = os.path.join(spectrogram_folder, label_folder)\n",
        "\n",
        "            for image_name in tqdm(os.listdir(output_dir)):\n",
        "                image_path = os.path.join(output_dir, image_name)\n",
        "                image_tensor = self.image_to_tensor(image_path)\n",
        "\n",
        "                tensors.append(image_tensor)\n",
        "                labels.append(label_value)\n",
        "\n",
        "        # Tensor of all features (N x D) - N is number of samples (377), D is feature dimension (3,224,224)\n",
        "        X = torch.stack(tensors)\n",
        "        # Tensor of all labels (N x 1) - 377x1\n",
        "        y = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "        return TensorDataset(X, y)\n",
        "\n",
        "\n",
        "    def image_to_tensor(self, image_path: str) -> torch.Tensor:\n",
        "        \"\"\"Converts a spectrogram image to a PyTorch tensor.\n",
        "\n",
        "        Args:\n",
        "            image_path (str): Path to the spectrogram image file.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The PyTorch tensor representation of the image.\n",
        "        \"\"\"\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),  # Resize to ResNet18 input size\n",
        "            transforms.ToTensor(),  # Convert image to tensor\n",
        "        ])\n",
        "\n",
        "        image = Image.open(image_path).convert(\"RGB\") # Convert from RGBA to RGB\n",
        "        tensor_image = transform(image)\n",
        "\n",
        "        return tensor_image  # shape will be 3, 224, 224\n",
        "\n",
        "    def create_dataloaders(self, batch_size, dataset_path=None, upsample=True):\n",
        "        if dataset_path:\n",
        "            dataset = torch.load(dataset_path, weights_only=False)\n",
        "        else:\n",
        "            dataset = self.load_dataset()\n",
        "\n",
        "        test_size = round(self.test_size * len(dataset))\n",
        "        val_size = round(self.val_size * len(dataset))\n",
        "        train_size = len(dataset) - test_size - val_size\n",
        "\n",
        "        train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "        train_labels_flat = []\n",
        "\n",
        "        if upsample:\n",
        "            labels = [label.item() for _, label in train_dataset]\n",
        "            train_labels_flat = labels\n",
        "\n",
        "            train_counts = {label: labels.count(label) for label in set(labels)}\n",
        "            weights = torch.tensor([1.0 / train_counts[label] for label in labels])\n",
        "            sampler = WeightedRandomSampler(weights, int(len(train_dataset) * 1.5))\n",
        "            train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
        "        else:\n",
        "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "            train_labels_flat = [label.item() for _, label in train_dataset]\n",
        "\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        return train_loader, val_loader, test_loader, train_labels_flat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "k3JgY0lnclQH"
      },
      "outputs": [],
      "source": [
        "import torch.optim as opt\n",
        "\n",
        "# Static hyperparameters\n",
        "EPOCHS = 20\n",
        "\n",
        "# Learning rate scheduler\n",
        "STEPS_PER_LR_DECAY = 20\n",
        "LR_DECAY = 0.5\n",
        "\n",
        "# Model parameters\n",
        "DROPOUT = 0.5\n",
        "\n",
        "# Training\n",
        "LOSS_FN = nn.BCEWithLogitsLoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rgLkS_upcm6n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "166a95a8-79b5-4dfb-f4a5-4829c35cda18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b3_rwightman-b3899882.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b3_rwightman-b3899882.pth\n",
            "100%|██████████| 47.2M/47.2M [00:00<00:00, 111MB/s]\n"
          ]
        }
      ],
      "source": [
        "model = CNNModel(DROPOUT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItAMI1yPcohx",
        "outputId": "5c8fbf94-475c-4e9c-ec94-bbde1b8a226d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1409/1409 [01:19<00:00, 17.78it/s]\n",
            "100%|██████████| 4274/4274 [04:47<00:00, 14.86it/s]\n"
          ]
        }
      ],
      "source": [
        "datapipeline = DataPipeline(test_size=0.15, val_size=0.15)\n",
        "train_loader, val_loader, test_loader, train_labels = datapipeline.create_dataloaders(batch_size=8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "yXMzHuqedyuC",
        "outputId": "27951647-fd40-4230-b315-e60b1b445940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🚀 Training with batch size: 16, learning rate: 0.0002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1409/1409 [00:39<00:00, 35.58it/s]\n",
            "  1%|          | 44/4274 [00:10<16:02,  4.39it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-0693bf33915b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Load dataset with the chosen batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mdatapipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatapipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Debug: Check dataset distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-d7a1880a32db>\u001b[0m in \u001b[0;36mcreate_dataloaders\u001b[0;34m(self, batch_size, dataset_path, upsample)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-d7a1880a32db>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mimage_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mimage_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-d7a1880a32db>\u001b[0m in \u001b[0;36mimage_to_tensor\u001b[0;34m(self, image_path)\u001b[0m\n\u001b[1;32m     62\u001b[0m         ])\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Convert from RGBA to RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mtensor_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3465\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3466\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3467\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch.optim as opt\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torchaudio.transforms as T\n",
        "import torch.nn.utils\n",
        "import numpy as np\n",
        "\n",
        "# Define fixed hyperparameters\n",
        "batch_size = 16  # Reduced batch size for better generalization\n",
        "learning_rate = 0.0002  # More stable learning rate\n",
        "weight_decay = 5e-4  # Regularization strength\n",
        "dropout_rate = 0.6  # Increased dropout to reduce overfitting\n",
        "patience = 15  # Increased patience for early stopping\n",
        "\n",
        "print(f\"\\n🚀 Training with batch size: {batch_size}, learning rate: {learning_rate}\")\n",
        "\n",
        "# Initialize model (Use custom CNNModel with EfficientNet-B0 backbone)\n",
        "cnn_model = CNNModel(dropout=dropout_rate)\n",
        "\n",
        "# Choose optimizer (SGD with momentum for better generalization)\n",
        "optimizer = opt.SGD(params=cnn_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
        "\n",
        "# Learning rate scheduler (CosineAnnealingLR for smooth LR decay)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=20)\n",
        "\n",
        "# Data Augmentation (SpecAugment with additional Mixup)\n",
        "def augment_spectrogram(spectrogram):\n",
        "    spectrogram = T.FrequencyMasking(freq_mask_param=15)(spectrogram)\n",
        "    spectrogram = T.TimeMasking(time_mask_param=25)(spectrogram)\n",
        "    spectrogram = T.Vol(0.8)(spectrogram)  # Random volume adjustment\n",
        "    return spectrogram\n",
        "\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    index = torch.randperm(x.size(0))\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "# Create ModelHandler\n",
        "model_handler = ModelHandler(model=cnn_model,\n",
        "                             model_path=\"/content/gdrive/MyDrive/RespiraCheck/Cough Data\",\n",
        "                             optimizer=optimizer,\n",
        "                             loss_function=LOSS_FN,\n",
        "                             steps_per_decay=STEPS_PER_LR_DECAY,\n",
        "                             lr_decay=LR_DECAY)\n",
        "\n",
        "# Load dataset with the chosen batch size\n",
        "datapipeline = DataPipeline(test_size=0.15, val_size=0.15)\n",
        "train_loader, val_loader, test_loader = datapipeline.create_dataloaders(batch_size=batch_size, upsample=True)\n",
        "\n",
        "# Debug: Check dataset distribution\n",
        "print(\"\\n📊 Dataset Split:\")\n",
        "print(f\"- Training Samples: {len(train_loader.dataset)}\")\n",
        "print(f\"- Validation Samples: {len(val_loader.dataset)}\")\n",
        "print(f\"- Test Samples: {len(test_loader.dataset)}\")\n",
        "\n",
        "# Training loop with early stopping\n",
        "best_val_loss = float(\"inf\")\n",
        "best_model = None\n",
        "best_acc = 0.0\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n🔄 Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    # Train\n",
        "    train_results, val_results = model_handler.train(train_loader=train_loader, epochs=1, model_name=\"CNN_EfficientNet\")\n",
        "    train_loss = train_results[\"loss\"][-1]  # Get the last recorded loss value\n",
        "\n",
        "    # Validate\n",
        "    val_acc, val_loss = model_handler.validate(val_loader, {\"batch_size\": batch_size, \"lr\": learning_rate})\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step()\n",
        "\n",
        "    # Gradient Clipping to stabilize training\n",
        "    torch.nn.utils.clip_grad_norm_(cnn_model.parameters(), max_norm=1.0)\n",
        "\n",
        "    # Check if validation loss improved\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_acc = val_acc\n",
        "        best_model = model_handler\n",
        "        epochs_without_improvement = 0  # Reset counter if there's improvement\n",
        "        print(f\"✅ New best validation loss: {best_val_loss:.4f} | Validation accuracy: {val_acc*100:.2f}%\")\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        print(f\"🔄 No improvement in validation loss for {epochs_without_improvement} epochs\")\n",
        "\n",
        "    # Early Stopping: Stop training if there's no improvement for `patience` epochs\n",
        "    if epochs_without_improvement >= patience:\n",
        "        print(f\"⏹️ Early stopping triggered due to no improvement in validation loss.\")\n",
        "        break\n",
        "\n",
        "# Final Testing\n",
        "if best_model:\n",
        "    test_acc = best_model.evaluate(test_loader)\n",
        "    print(f\"\\n🎯 Test accuracy: {test_acc*100:.2f}% 🚀 Best model saved!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwlwx85BhN3s",
        "outputId": "f1a00423-8823-4020-dc45-37fc33054df5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🚀 Training with batch size: 16, learning rate: 0.0002\n",
            "Processing and loading dataset\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1409/1409 [00:27<00:00, 50.32it/s]\n",
            "100%|██████████| 4274/4274 [01:44<00:00, 41.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upsampling data\n",
            "\n",
            "📊 Dataset Split:\n",
            "- Training Samples: 3979\n",
            "- Validation Samples: 852\n",
            "- Test Samples: 852\n",
            "\n",
            "🔄 Epoch 1/20\n",
            "0:\n",
            "LR: 0.0003\n",
            "Loss - 0.58544 | Accuracy - 68.62%\n",
            "VLoss - 0.69999 | VAccuracy - 65.86%\n",
            "\n",
            "📊 Validation Accuracy: 75.00% | F1-score: 0.6429\n",
            "✅ New best F1-score: 0.6429\n",
            "\n",
            "🔄 Epoch 2/20\n",
            "0:\n",
            "LR: 0.00029815325108927063\n",
            "Loss - 0.38090 | Accuracy - 83.85%\n",
            "VLoss - 0.85533 | VAccuracy - 60.42%\n",
            "\n",
            "📊 Validation Accuracy: 75.00% | F1-score: 0.6429\n",
            "🔄 No improvement in F1-score for 1 epochs\n",
            "\n",
            "🔄 Epoch 3/20\n",
            "0:\n",
            "LR: 0.00029265847744427303\n",
            "Loss - 0.23048 | Accuracy - 91.09%\n",
            "VLoss - 0.85182 | VAccuracy - 68.29%\n",
            "\n",
            "📊 Validation Accuracy: 75.00% | F1-score: 0.6429\n",
            "🔄 No improvement in F1-score for 2 epochs\n",
            "\n",
            "🔄 Epoch 4/20\n",
            "0:\n",
            "LR: 0.0002836509786282552\n",
            "Loss - 0.14031 | Accuracy - 94.44%\n",
            "VLoss - 1.08216 | VAccuracy - 67.59%\n",
            "\n",
            "📊 Validation Accuracy: 75.00% | F1-score: 0.6429\n",
            "🔄 No improvement in F1-score for 3 epochs\n",
            "\n",
            "🔄 Epoch 5/20\n",
            "0:\n",
            "LR: 0.0002713525491562421\n",
            "Loss - 0.11338 | Accuracy - 96.15%\n",
            "VLoss - 1.18663 | VAccuracy - 72.11%\n",
            "\n",
            "📊 Validation Accuracy: 75.00% | F1-score: 0.6429\n",
            "🔄 No improvement in F1-score for 4 epochs\n",
            "\n",
            "🔄 Epoch 6/20\n",
            "0:\n",
            "LR: 0.0002560660171779821\n",
            "Loss - 0.08236 | Accuracy - 97.13%\n",
            "VLoss - 1.32689 | VAccuracy - 68.40%\n",
            "\n",
            "📊 Validation Accuracy: 75.00% | F1-score: 0.6429\n",
            "🔄 No improvement in F1-score for 5 epochs\n",
            "\n",
            "🔄 Epoch 7/20\n",
            "0:\n",
            "LR: 0.000238167787843871\n",
            "Loss - 0.05962 | Accuracy - 97.86%\n",
            "VLoss - 1.78482 | VAccuracy - 72.45%\n",
            "\n",
            "📊 Validation Accuracy: 75.00% | F1-score: 0.6429\n",
            "🔄 No improvement in F1-score for 6 epochs\n",
            "\n",
            "🔄 Epoch 8/20\n",
            "0:\n",
            "LR: 0.00021809857496093204\n",
            "Loss - 0.04270 | Accuracy - 98.58%\n",
            "VLoss - 1.63446 | VAccuracy - 72.69%\n",
            "\n",
            "📊 Validation Accuracy: 75.00% | F1-score: 0.6429\n",
            "🔄 No improvement in F1-score for 7 epochs\n",
            "\n",
            "🔄 Epoch 9/20\n",
            "0:\n",
            "LR: 0.00019635254915624216\n",
            "Loss - 0.03785 | Accuracy - 98.74%\n",
            "VLoss - 1.76318 | VAccuracy - 71.53%\n",
            "\n",
            "📊 Validation Accuracy: 75.00% | F1-score: 0.6429\n",
            "🔄 No improvement in F1-score for 8 epochs\n",
            "\n",
            "🔄 Epoch 10/20\n",
            "0:\n",
            "LR: 0.00017346516975603468\n",
            "Loss - 0.01954 | Accuracy - 99.23%\n",
            "VLoss - 1.81697 | VAccuracy - 73.15%\n",
            "\n",
            "📊 Validation Accuracy: 75.00% | F1-score: 0.6429\n",
            "🔄 No improvement in F1-score for 9 epochs\n",
            "\n",
            "🔄 Epoch 11/20\n",
            "0:\n",
            "LR: 0.00015000000000000001\n",
            "Loss - 0.02299 | Accuracy - 99.28%\n",
            "VLoss - 1.55185 | VAccuracy - 71.18%\n",
            "\n",
            "📊 Validation Accuracy: 75.00% | F1-score: 0.6429\n",
            "🔄 No improvement in F1-score for 10 epochs\n",
            "\n",
            "🔄 Epoch 12/20\n",
            "0:\n",
            "LR: 0.0001265348302439654\n",
            "Loss - 0.01627 | Accuracy - 99.45%\n",
            "VLoss - 1.87475 | VAccuracy - 73.61%\n",
            "\n",
            "📊 Validation Accuracy: 75.00% | F1-score: 0.6429\n",
            "🔄 No improvement in F1-score for 11 epochs\n",
            "\n",
            "🔄 Epoch 13/20\n",
            "0:\n",
            "LR: 0.0001036474508437579\n",
            "Loss - 0.00982 | Accuracy - 99.73%\n",
            "VLoss - 1.75858 | VAccuracy - 74.54%\n",
            "\n",
            "📊 Validation Accuracy: 75.00% | F1-score: 0.6429\n",
            "🔄 No improvement in F1-score for 12 epochs\n",
            "\n",
            "🔄 Epoch 14/20\n",
            "0:\n",
            "LR: 8.190142503906799e-05\n",
            "Loss - 0.00771 | Accuracy - 99.77%\n",
            "VLoss - 1.82121 | VAccuracy - 74.07%\n",
            "\n",
            "📊 Validation Accuracy: 75.00% | F1-score: 0.6429\n",
            "🔄 No improvement in F1-score for 13 epochs\n",
            "\n",
            "🔄 Epoch 15/20\n",
            "0:\n",
            "LR: 6.183221215612905e-05\n",
            "Loss - 0.00698 | Accuracy - 99.82%\n",
            "VLoss - 1.81705 | VAccuracy - 72.80%\n",
            "\n",
            "📊 Validation Accuracy: 75.00% | F1-score: 0.6429\n",
            "🔄 No improvement in F1-score for 14 epochs\n",
            "\n",
            "🔄 Epoch 16/20\n",
            "0:\n",
            "LR: 4.393398282201788e-05\n",
            "Loss - 0.00432 | Accuracy - 99.93%\n",
            "VLoss - 1.93557 | VAccuracy - 73.61%\n",
            "\n",
            "📊 Validation Accuracy: 75.00% | F1-score: 0.6429\n",
            "🔄 No improvement in F1-score for 15 epochs\n",
            "⏹️ Early stopping triggered due to no improvement in F1-score.\n",
            "\n",
            "🎯 Test Accuracy: 73.47% | Test F1-score: 0.0000 🚀\n",
            "\n",
            "🎯 Test Accuracy: 73.47% | Test F1-score: 0.0000 🚀 Best model saved!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as opt\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torchaudio.transforms as T\n",
        "import torch.nn.utils\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "# Define fixed hyperparameters\n",
        "batch_size = 16  # Reduced batch size for better generalization\n",
        "learning_rate = 0.0002  # More stable learning rate\n",
        "weight_decay = 5e-4  # Regularization strength\n",
        "dropout_rate = 0.6  # Increased dropout to reduce overfitting\n",
        "patience = 15  # Early stopping patience\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\\n🚀 Training with batch size: {batch_size}, learning rate: {learning_rate}\")\n",
        "\n",
        "# Initialize model (Use custom CNNModel with EfficientNet-B0 backbone)\n",
        "cnn_model = CNNModel(dropout=dropout_rate).to(device)  # Move model to device\n",
        "\n",
        "# Choose optimizer (SGD with momentum for better generalization)\n",
        "optimizer = opt.SGD(params=cnn_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
        "\n",
        "# Learning rate scheduler (CosineAnnealingLR for smooth LR decay)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=20)\n",
        "\n",
        "# Log-Mel Spectrogram transformation\n",
        "sample_rate = 16000  # Adjust based on your dataset's sample rate\n",
        "n_fft = 400  # Number of samples per FFT\n",
        "win_length = 400  # Length of the window\n",
        "hop_length = 160  # Hop length (the number of samples between successive frames)\n",
        "n_mels = 23  # Number of Mel bins\n",
        "f_min = 0  # Minimum frequency\n",
        "f_max = sample_rate // 2  # Maximum frequency (Nyquist)\n",
        "\n",
        "# Log-Mel Spectrogram transformation\n",
        "log_mel_transform = T.MelSpectrogram(\n",
        "    sample_rate=sample_rate,\n",
        "    n_fft=n_fft,\n",
        "    win_length=win_length,\n",
        "    hop_length=hop_length,\n",
        "    n_mels=n_mels,\n",
        "    f_min=f_min,\n",
        "    f_max=f_max\n",
        ")\n",
        "\n",
        "# Logarithmic compression\n",
        "log_transform = T.AmplitudeToDB(stype='power')\n",
        "\n",
        "# Function to compute Log-Mel Spectrogram\n",
        "def compute_log_mel_spectrogram(audio_waveform):\n",
        "    mel_spectrogram = log_mel_transform(audio_waveform)\n",
        "    log_mel_spectrogram = log_transform(mel_spectrogram)\n",
        "    return log_mel_spectrogram\n",
        "\n",
        "# Data Augmentation (SpecAugment + Mixup)\n",
        "def augment_spectrogram(spectrogram):\n",
        "    # Apply Log-Mel Spectrogram transformation here\n",
        "    spectrogram = compute_log_mel_spectrogram(spectrogram)\n",
        "\n",
        "    # SpecAugment augmentations\n",
        "    spectrogram = T.FrequencyMasking(freq_mask_param=15)(spectrogram)  # Frequency Masking\n",
        "    spectrogram = T.TimeMasking(time_mask_param=25)(spectrogram)  # Time Masking\n",
        "    spectrogram = T.Vol(0.8)(spectrogram)  # Random volume adjustment\n",
        "\n",
        "    return spectrogram\n",
        "\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    index = torch.randperm(x.size(0))\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "# Create ModelHandler\n",
        "model_handler = ModelHandler(\n",
        "    model=cnn_model,\n",
        "    model_path=\"/content/gdrive/MyDrive/RespiraCheck/Cough Data\",\n",
        "    optimizer=optimizer,\n",
        "    loss_function=LOSS_FN,\n",
        "    steps_per_decay=STEPS_PER_LR_DECAY,\n",
        "    lr_decay=LR_DECAY\n",
        ")\n",
        "\n",
        "# Load dataset with the chosen batch size\n",
        "datapipeline = DataPipeline(test_size=0.15, val_size=0.15)\n",
        "train_loader, val_loader, test_loader = datapipeline.create_dataloaders(batch_size=batch_size, upsample=True)\n",
        "\n",
        "# Debug: Check dataset distribution\n",
        "print(\"\\n📊 Dataset Split:\")\n",
        "print(f\"- Training Samples: {len(train_loader.dataset)}\")\n",
        "print(f\"- Validation Samples: {len(val_loader.dataset)}\")\n",
        "print(f\"- Test Samples: {len(test_loader.dataset)}\")\n",
        "\n",
        "# Function to Validate and Compute F1-score\n",
        "def validate(model_handler, val_loader):\n",
        "    model_handler.model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.to(device), targets.to(device)  # Move to device\n",
        "            outputs = model_handler.model(inputs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    val_acc = accuracy_score(all_targets, all_preds)\n",
        "    val_f1 = f1_score(all_targets, all_preds, average=\"weighted\")\n",
        "\n",
        "    return val_acc, val_f1\n",
        "\n",
        "# Training loop with early stopping\n",
        "best_val_loss = float(\"inf\")\n",
        "best_model = None\n",
        "best_acc = 0.0\n",
        "best_f1_score = 0.0\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n🔄 Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    # Train\n",
        "    train_results, val_results = model_handler.train(train_loader=train_loader, epochs=1, model_name=\"CNN_EfficientNet\")\n",
        "\n",
        "    # Validate and Compute F1-score\n",
        "    val_acc, val_f1 = validate(model_handler, val_loader)\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"📊 Validation Accuracy: {val_acc*100:.2f}% | F1-score: {val_f1:.4f}\")\n",
        "\n",
        "    # Check for improvement\n",
        "    if val_f1 > best_f1_score:\n",
        "        best_f1_score = val_f1\n",
        "        best_model = model_handler\n",
        "        epochs_without_improvement = 0  # Reset counter if there's improvement\n",
        "        print(f\"✅ New best F1-score: {best_f1_score:.4f}\")\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        print(f\"🔄 No improvement in F1-score for {epochs_without_improvement} epochs\")\n",
        "\n",
        "    # Early Stopping\n",
        "    if epochs_without_improvement >= patience:\n",
        "        print(f\"⏹️ Early stopping triggered due to no improvement in F1-score.\")\n",
        "        break\n",
        "\n",
        "# Function to Evaluate Model on Test Set\n",
        "def evaluate(model_handler, test_loader):\n",
        "    model_handler.model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.to(device), targets.to(device)  # Move to device\n",
        "            outputs = model_handler.model(inputs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    test_acc = accuracy_score(all_targets, all_preds)\n",
        "    test_f1 = f1_score(all_targets, all_preds, average=\"weighted\")\n",
        "\n",
        "    print(f\"\\n🎯 Test Accuracy: {test_acc*100:.2f}% | Test F1-score: {test_f1:.4f} 🚀\")\n",
        "    return test_acc, test_f1\n",
        "\n",
        "# Final Testing\n",
        "if best_model:\n",
        "    test_acc, test_f1 = evaluate(best_model, test_loader)\n",
        "    print(f\"\\n🎯 Test Accuracy: {test_acc*100:.2f}% | Test F1-score: {test_f1:.4f} 🚀 Best model saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4AJt99S0XST",
        "outputId": "89d84154-f03d-44d7-816f-3d75330a5c2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Training with batch size: 16, learning rate: 0.0002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1409/1409 [00:38<00:00, 36.22it/s]\n",
            "100%|██████████| 4274/4274 [02:18<00:00, 30.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Epoch 1/20\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as opt\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torchaudio.transforms as T\n",
        "import torch.nn.utils\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Fixed hyperparameters\n",
        "batch_size = 16\n",
        "learning_rate = 0.0002\n",
        "weight_decay = 5e-4\n",
        "dropout_rate = 0.6\n",
        "patience = 15\n",
        "EPOCHS = 20\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\\n🚀 Training with batch size: {batch_size}, learning rate: {learning_rate}\")\n",
        "\n",
        "# Model init\n",
        "cnn_model = CNNModel(dropout=dropout_rate).to(device)\n",
        "optimizer = torch.optim.AdamW(cnn_model.parameters(), lr=0.0003, weight_decay=weight_decay)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=20)\n",
        "\n",
        "# Log-Mel Spectrogram transformation\n",
        "sample_rate = 16000\n",
        "n_fft = 400\n",
        "win_length = 400\n",
        "hop_length = 160\n",
        "n_mels = 23\n",
        "f_min = 0\n",
        "f_max = sample_rate // 2\n",
        "\n",
        "log_mel_transform = T.MelSpectrogram(\n",
        "    sample_rate=sample_rate,\n",
        "    n_fft=n_fft,\n",
        "    win_length=win_length,\n",
        "    hop_length=hop_length,\n",
        "    n_mels=n_mels,\n",
        "    f_min=f_min,\n",
        "    f_max=f_max\n",
        ")\n",
        "log_transform = T.AmplitudeToDB(stype='power')\n",
        "\n",
        "def compute_log_mel_spectrogram(audio_waveform):\n",
        "    mel_spectrogram = log_mel_transform(audio_waveform)\n",
        "    log_mel_spectrogram = log_transform(mel_spectrogram)\n",
        "    return log_mel_spectrogram\n",
        "\n",
        "def augment_spectrogram(spectrogram):\n",
        "    spectrogram = compute_log_mel_spectrogram(spectrogram)\n",
        "    spectrogram = T.FrequencyMasking(freq_mask_param=10)(spectrogram)\n",
        "    spectrogram = T.TimeMasking(time_mask_param=20)(spectrogram)\n",
        "    return spectrogram\n",
        "\n",
        "# Load data\n",
        "datapipeline = DataPipeline(test_size=0.15, val_size=0.15)\n",
        "train_loader, val_loader, test_loader, all_train_labels = datapipeline.create_dataloaders(batch_size=batch_size, upsample=True)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(all_train_labels), y=all_train_labels)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "\n",
        "# Define loss\n",
        "loss_function = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "# Initialize model handler\n",
        "model_handler = ModelHandler(\n",
        "    model=cnn_model,\n",
        "    model_path=\"/content/gdrive/MyDrive/RespiraCheck/Cough Data\",\n",
        "    optimizer=optimizer,\n",
        "    loss_function=loss_function,\n",
        "    steps_per_decay=STEPS_PER_LR_DECAY,\n",
        "    lr_decay=LR_DECAY\n",
        ")\n",
        "\n",
        "# Validation function\n",
        "def validate(model_handler, val_loader):\n",
        "    model_handler.model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model_handler.model(inputs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_targets = np.array(all_targets)\n",
        "\n",
        "    val_acc = accuracy_score(all_targets, all_preds)\n",
        "\n",
        "    tp = np.sum((all_preds == 1) & (all_targets == 1))\n",
        "    fp = np.sum((all_preds == 1) & (all_targets == 0))\n",
        "    fn = np.sum((all_preds == 0) & (all_targets == 1))\n",
        "    tn = np.sum((all_preds == 0) & (all_targets == 0))\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0.0\n",
        "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0.0\n",
        "    val_f1 = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0.0\n",
        "\n",
        "    print(f\"📊 Validation Accuracy: {val_acc*100:.2f}% | F1-score: {val_f1:.4f}\")\n",
        "    print(f\"TP: {tp} | FP: {fp} | FN: {fn} | TN: {tn} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "    return val_acc, val_f1\n",
        "\n",
        "# Evaluation\n",
        "def evaluate(model_handler, test_loader):\n",
        "    model_handler.model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model_handler.model(inputs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_targets = np.array(all_targets)\n",
        "\n",
        "    test_acc = accuracy_score(all_targets, all_preds)\n",
        "\n",
        "    tp = np.sum((all_preds == 1) & (all_targets == 1))\n",
        "    fp = np.sum((all_preds == 1) & (all_targets == 0))\n",
        "    fn = np.sum((all_preds == 0) & (all_targets == 1))\n",
        "    tn = np.sum((all_preds == 0) & (all_targets == 0))\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0.0\n",
        "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0.0\n",
        "    test_f1 = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0.0\n",
        "\n",
        "    print(f\"\\n🎯 Test Accuracy: {test_acc*100:.2f}% | Test F1-score: {test_f1:.4f} 🚀\")\n",
        "    print(f\"TP: {tp} | FP: {fp} | FN: {fn} | TN: {tn} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "    return test_acc, test_f1\n",
        "\n",
        "# Training loop\n",
        "best_f1_score = 0.0\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n🔄 Epoch {epoch+1}/{EPOCHS}\")\n",
        "   # for batch in train_loader:\n",
        "   #     inputs, targets = batch\n",
        "   #     inputs = augment_spectrogram(inputs).to(device)\n",
        "   #     targets = targets.to(device)\n",
        "\n",
        "    train_results, val_results = model_handler.train(train_loader=train_loader, val_loader=val_loader,epochs=1, model_name=\"CNN_EfficientNet\")\n",
        "    val_acc, val_f1 = validate(model_handler, val_loader)\n",
        "    scheduler.step()\n",
        "\n",
        "    if val_f1 > best_f1_score:\n",
        "        best_f1_score = val_f1\n",
        "        best_model = model_handler\n",
        "        epochs_without_improvement = 0\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "\n",
        "    if epochs_without_improvement >= patience:\n",
        "        print(f\"⏹️ Early stopping triggered due to no improvement in F1-score.\")\n",
        "        break\n",
        "\n",
        "# Final evaluation\n",
        "if best_model:\n",
        "    test_acc, test_f1 = evaluate(best_model, test_loader)\n",
        "    print(f\"\\n🎯 Test Accuracy: {test_acc*100:.2f}% | Test F1-score: {test_f1:.4f} 🚀 Best model saved!\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}