{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gYscyCg6b7rZ"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import collections\n",
        "import time\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import random_split, DataLoader, TensorDataset, WeightedRandomSampler\n",
        "from torchvision import transforms\n",
        "import io\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0AUU9kYcUAv",
        "outputId": "99f47133-50b0-4efe-9d01-2962ed643ae6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "['cough_data']\n",
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "print(os.listdir('/content/gdrive/MyDrive/RespiraCheck'))\n",
        "print(os.path.exists('/content/gdrive/MyDrive/RespiraCheck/cough_data/spectrograms_houman/positive'))\n",
        "print(os.path.exists('/content/gdrive/MyDrive/RespiraCheck/cough_data/spectrograms_houman/negative'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tbLR_B94cdPR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    \"\"\"Convolutional Block Attention Module (CBAM) for feature refinement.\"\"\"\n",
        "    def __init__(self, channels, reduction=16, kernel_size=7):\n",
        "        super(CBAM, self).__init__()\n",
        "\n",
        "        # Channel Attention Module\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.fc1 = nn.Linear(channels, channels // reduction, bias=False)\n",
        "        self.fc2 = nn.Linear(channels // reduction, channels, bias=False)\n",
        "        self.sigmoid_channel = nn.Sigmoid()\n",
        "\n",
        "        # Spatial Attention Module\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2, bias=False)\n",
        "        self.sigmoid_spatial = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Channel attention\n",
        "        avg_out = self.fc2(torch.relu(self.fc1(self.avg_pool(x).view(x.shape[0], -1))))\n",
        "        max_out = self.fc2(torch.relu(self.fc1(self.max_pool(x).view(x.shape[0], -1))))\n",
        "        channel_attention = self.sigmoid_channel(avg_out + max_out).view(x.shape[0], x.shape[1], 1, 1)\n",
        "        x = x * channel_attention\n",
        "\n",
        "        # Spatial attention\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        spatial_attention = self.sigmoid_spatial(self.conv(torch.cat([avg_out, max_out], dim=1)))\n",
        "        x = x * spatial_attention\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class CNN_CBAMModel(nn.Module):\n",
        "    \"\"\"A convolutional neural network model with CBAM for enhanced feature extraction.\"\"\"\n",
        "\n",
        "    def __init__(self, dropout: float = 0.0):\n",
        "        \"\"\"Initializes the CNNModel using EfficientNet-B0 with CBAM.\n",
        "\n",
        "        Args:\n",
        "            dropout (float): Dropout probability before the final classification layer.\n",
        "        \"\"\"\n",
        "        super(CNN_CBAMModel, self).__init__()\n",
        "\n",
        "        # Load EfficientNet-B0 with pre-trained weights\n",
        "        self.efficientnet = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
        "\n",
        "        # Add CBAM module after feature extraction layers\n",
        "        self.cbam = CBAM(channels=1280)  # EfficientNet-B0 last feature map has 1280 channels\n",
        "\n",
        "        # Get the number of features from the last layer of EfficientNet\n",
        "        num_features = self.efficientnet.classifier[1].in_features\n",
        "\n",
        "        # Replace the classifier with a new sequence including Dropout and FC layer\n",
        "        self.efficientnet.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(num_features, 1)  # Binary classification output\n",
        "        )\n",
        "\n",
        "        # Initialize the new FC layer weights\n",
        "        nn.init.normal_(self.efficientnet.classifier[1].weight, mean=0.0, std=0.01)\n",
        "        nn.init.zeros_(self.efficientnet.classifier[1].bias)\n",
        "\n",
        "    def forward(self, spectrogram: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Defines the forward pass for EfficientNet with CBAM.\n",
        "\n",
        "        Args:\n",
        "            spectrogram (torch.Tensor): Input tensor representing the spectrogram.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The model's output (logit for binary classification).\n",
        "        \"\"\"\n",
        "        features = self.efficientnet.features(spectrogram)\n",
        "        features = self.cbam(features)  # Apply CBAM attention\n",
        "        pooled = self.efficientnet.avgpool(features)\n",
        "        flattened = torch.flatten(pooled, 1)\n",
        "        output = self.efficientnet.classifier(flattened)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9ydySeiGchks"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as opt\n",
        "import numpy as np\n",
        "import collections\n",
        "import os\n",
        "import time\n",
        "\n",
        "class ModelHandler:\n",
        "    \"\"\"Handles the model training, evaluation, and inference pipeline.\n",
        "\n",
        "    Attributes:\n",
        "        device (torch.device): The device on which the model is executed (e.g., 'cpu' or 'cuda').\n",
        "        model_path: Path to where .pth models should be saved.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 model_path: str,\n",
        "                 optimizer: torch.optim.Optimizer,\n",
        "                 loss_function: nn.Module,\n",
        "                 steps_per_decay = 5,\n",
        "                 lr_decay = 0.1):\n",
        "        \"\"\"Initializes the ModelHandler.\n",
        "\n",
        "        Args:\n",
        "            model_path (str | None): Path to the pre-trained model file (if available).\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model_path = model_path\n",
        "        self.optimizer = optimizer\n",
        "        self.lr_scheduler = opt.lr_scheduler.StepLR(self.optimizer, step_size=steps_per_decay, gamma=lr_decay)\n",
        "        self.loss_function = loss_function\n",
        "\n",
        "    def train_step(self, dataloader):\n",
        "        \"\"\"Trains the model for a single epoch.\n",
        "\n",
        "        Args:\n",
        "            dataloader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        avg_loss, acc = 0, 0\n",
        "        for in_tensor, labels in dataloader:\n",
        "            in_tensor, labels = in_tensor.to(self.device), labels.to(self.device)\n",
        "            labels = labels.float().unsqueeze(1)  # Ensure correct shape for BCE loss\n",
        "\n",
        "            logits = self.model(in_tensor) # Feed input into model\n",
        "\n",
        "            loss = self.loss_function(logits, labels)  # Calculate loss\n",
        "            avg_loss += loss.item()  # Add to cumulative loss\n",
        "\n",
        "            # Gradient descent\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Calculate batch accuracy and add it to cumulative accuracy\n",
        "            prediction_classes = torch.round(torch.sigmoid(logits))\n",
        "            batch_acc = torch.mean((prediction_classes == labels).float()).item()\n",
        "            acc += batch_acc\n",
        "\n",
        "        avg_loss /= len(dataloader)  # Calculate avg loss for epoch from cumulative loss\n",
        "        acc /= len(dataloader)  # Calculate avg accuracy for epoch from cumulative accuracy\n",
        "        train_results = {\"avg_loss_per_batch\": avg_loss, \"avg_acc_per_batch\": acc * 100}\n",
        "        return train_results\n",
        "\n",
        "    def val_step(self, dataloader):\n",
        "        \"\"\"Evaluates the model on the validation dataset.\n",
        "\n",
        "        Args:\n",
        "            dataloader (torch.utils.data.DataLoader): DataLoader for the validation dataset.\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.inference_mode():\n",
        "            avg_loss, acc = 0, 0\n",
        "            for in_tensor, labels in dataloader:\n",
        "                in_tensor, labels = in_tensor.to(self.device), labels.to(self.device)\n",
        "                labels = labels.float().unsqueeze(1)  # Ensure correct shape for BCE loss\n",
        "\n",
        "                logits = self.model(in_tensor)  # Feed input into model\n",
        "\n",
        "                loss = self.loss_function(logits, labels)  # Calculate loss\n",
        "                avg_loss += loss.item()  # Add to cumulative loss\n",
        "\n",
        "                # Calculate batch accuracy and add it to cumulative accuracy\n",
        "                prediction_classes = torch.round(torch.sigmoid(logits))\n",
        "                batch_acc = torch.mean((prediction_classes == labels).float()).item()\n",
        "                acc += batch_acc\n",
        "\n",
        "            avg_loss /= len(dataloader)  # Calculate avg loss for each epoch from cumulative loss\n",
        "            acc /= len(dataloader)  # Calculate avg accuracy for each epoch from cumulative accuracy\n",
        "            valid_results = {\"avg_loss_per_batch\": avg_loss, \"avg_acc_per_batch\": acc * 100}\n",
        "            return valid_results\n",
        "\n",
        "    def train(self, train_loader, epochs: int, model_name: str):\n",
        "        \"\"\"Trains the model\n",
        "\n",
        "        Args:\n",
        "            train_loader: DataLoader for the training datasets\n",
        "            epochs (int): Number of training epochs.\n",
        "            model_name (str): Name to save the trained model.\n",
        "        \"\"\"\n",
        "        self.model.to(self.device)\n",
        "        training_results = {\"epoch\": [], \"loss\": [], \"accuracy\": []}\n",
        "        validation_results = {\"epoch\": [], \"loss\": [], \"accuracy\": []}\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # Train the model\n",
        "            training_data = self.train_step(train_loader)\n",
        "            training_results[\"epoch\"].append(epoch)\n",
        "            training_results[\"loss\"].append(training_data[\"avg_loss_per_batch\"])\n",
        "            training_results[\"accuracy\"].append(training_data[\"avg_acc_per_batch\"])\n",
        "\n",
        "            # Check the validation loss after training\n",
        "            validation_data = self.val_step(val_loader)\n",
        "            validation_results[\"epoch\"].append(epoch)\n",
        "            validation_results[\"loss\"].append(validation_data[\"avg_loss_per_batch\"])\n",
        "            validation_results[\"accuracy\"].append(validation_data[\"avg_acc_per_batch\"])\n",
        "\n",
        "            # Adjust learning rate if necessary\n",
        "            if self.lr_scheduler:\n",
        "                self.lr_scheduler.step()\n",
        "\n",
        "            if epoch % 1 == 0:\n",
        "                print(f\"{epoch}:\")\n",
        "                print(f\"LR: {self.optimizer.param_groups[0]['lr']}\")\n",
        "                print(f\"Loss - {training_data['avg_loss_per_batch']:.5f} | Accuracy - {training_data['avg_acc_per_batch']:.2f}%\")\n",
        "                print(f\"VLoss - {validation_data['avg_loss_per_batch']:.5f} | VAccuracy - {validation_data['avg_acc_per_batch']:.2f}%\\n\")\n",
        "\n",
        "        self.save_model(model_state_dict=self.model.state_dict(), model_name=model_name)\n",
        "        return training_results, validation_results\n",
        "\n",
        "\n",
        "    def validate(self, val_loader, hyperparams: dict, save_best: bool = True) -> tuple[float, float]:\n",
        "        \"\"\"Validates the model on the validation dataset.\n",
        "\n",
        "        Args:\n",
        "            val_loader: DataLoader for the validation dataset.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (validation accuracy, validation loss)\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        val_losses_epoch, batch_sizes, accs = [], [], []\n",
        "        best_acc = -1\n",
        "        best_model_state = None  # Track the best model weights\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X_val, y_val in val_loader:\n",
        "                X_val = X_val.to(self.device)\n",
        "                y_val = y_val.to(self.device).float().unsqueeze(1)\n",
        "\n",
        "                y_prediction_val = self.model(X_val)  # forward pass\n",
        "                loss = self.loss_function(y_prediction_val, y_val)\n",
        "                val_losses_epoch.append(loss.item())\n",
        "\n",
        "                # Compute accuracy\n",
        "                y_prediction_val = torch.sigmoid(y_prediction_val)  # Convert logits to probabilities\n",
        "                prediction_classes = (y_prediction_val > 0.5).float()  # Convert to binary 0/1\n",
        "\n",
        "                acc = torch.mean((prediction_classes == y_val).float()).item()\n",
        "                accs.append(acc)\n",
        "                batch_sizes.append(X_val.shape[0])\n",
        "\n",
        "        # Compute final validation loss and accuracy\n",
        "        val_loss = np.mean(val_losses_epoch)\n",
        "        val_acc = np.average(accs, weights=batch_sizes)  # Weighted average accuracy\n",
        "\n",
        "        print(f'Validation accuracy: {val_acc*100:.2f}% | Validation loss: {val_loss:.4f}')\n",
        "\n",
        "        if save_best and val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_model_state = self.model.state_dict()\n",
        "\n",
        "            # Create model filename using hyperparameters\n",
        "            hyperparam_str = \"_\".join(f\"{key}:{value}\" for key, value in hyperparams.items())\n",
        "            model_filename = f\"model_{hyperparam_str}_{time.time()}.pth\"\n",
        "\n",
        "            # Save the best model\n",
        "            save_path = os.path.join(self.model_path, model_filename)\n",
        "            torch.save(best_model_state, save_path)\n",
        "            print(f\"Best model saved at: {save_path}\")\n",
        "        return val_acc, val_loss\n",
        "\n",
        "\n",
        "    def evaluate(self, test_loader) -> float:\n",
        "        \"\"\"Evaluates the model on the test dataset.\n",
        "\n",
        "        Args:\n",
        "            test_loader: DataLoader for the test dataset.\n",
        "        \"\"\"\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        batch_sizes, accs = [], []\n",
        "        with torch.no_grad():\n",
        "            for X_test, y_test, in test_loader:\n",
        "                X_test = X_test.to(self.device)\n",
        "                y_test = y_test.to(self.device)\n",
        "\n",
        "                prediction = self.model(X_test)\n",
        "                batch_sizes.append(X_test.shape[0])\n",
        "\n",
        "                prediction = torch.sigmoid(prediction)\n",
        "                prediction_classes = (prediction > 0.5).float() # This converts to binary classes 0 and 1\n",
        "\n",
        "                acc = torch.mean((prediction_classes == y_test).float()).item()\n",
        "                accs.append(acc)\n",
        "\n",
        "        # Return average accuracy\n",
        "        return 0.0 if not accs else np.average(accs, weights=batch_sizes)\n",
        "\n",
        "\n",
        "    def predict(self, spectrogram: torch.Tensor, model_name: str) -> int:\n",
        "        \"\"\"Performs inference on a single spectrogram.\n",
        "\n",
        "        Args:\n",
        "            spectrogram (torch.Tensor): Input spectrogram for inference.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The predicted output from the model.\n",
        "        \"\"\"\n",
        "        self.load_model(self.model_path +f\"/{model_name}\")\n",
        "        spectrogram = spectrogram.unsqueeze(0).to(self.device)\n",
        "\n",
        "        with torch.no_grad:\n",
        "            logits = self.model(spectrogram)\n",
        "\n",
        "            probability = torch.sigmoid(logits)\n",
        "\n",
        "            prediction = (probability > 0.5).float() # Turn probability into binary classificaiton\n",
        "\n",
        "        return prediction.item()\n",
        "\n",
        "\n",
        "    def save_model(self, model_state_dict: collections.OrderedDict, model_name: str | None) -> None:\n",
        "        \"\"\"Saves the model to the specified file path.\n",
        "\n",
        "        Args:\n",
        "            path (str): Path to save the model file.\n",
        "        \"\"\"\n",
        "        path = self.model_path + \"/\" + model_name\n",
        "        torch.save(model_state_dict, path)\n",
        "\n",
        "\n",
        "    def load_model(self, path: str) -> None:\n",
        "        \"\"\"Loads a model from the specified file path.\n",
        "\n",
        "        Args:\n",
        "            path (str): Path to the model file.\n",
        "        \"\"\"\n",
        "        self.model.load_state_dict(torch.load(path))\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TtrsfN7QcjgM"
      },
      "outputs": [],
      "source": [
        "class DataPipeline:\n",
        "    \"\"\"Processes datasets, including loading, splitting, and preparing for inference.\n",
        "\n",
        "    This class provides methods for loading datasets, processing them for training,\n",
        "    and preparing single instances for inference.\n",
        "\n",
        "    Attributes:\n",
        "        test_size (float): Proportion of the dataset to include in the test split.\n",
        "        val_size (float): Proportion of the dataset to include for validation.\n",
        "        audio_processor: AudioProcessor instance for handling audio processing.\n",
        "        image_processor: ImageProcessor instance for handling spectrogram or extracted features processing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, test_size: float, val_size: float):\n",
        "        \"\"\"Initializes the DatasetProcessor.\n",
        "\n",
        "        Args:\n",
        "            data_path (str): Path to the dataset file.\n",
        "            test_size (float): Proportion of the dataset to include in the test split.\n",
        "            audio_processor (AudioProcessor): Instance for handling audio processing.\n",
        "            image_processor (ImageProcessor): Instance for handling spectrogram processing.\n",
        "        \"\"\"\n",
        "        self.test_size = test_size\n",
        "        self.val_size = val_size\n",
        "\n",
        "    def load_dataset(self) -> TensorDataset:\n",
        "        \"\"\"Loads the dataset from the specified file path into a DataFrame.\"\"\"\n",
        "        tensors = []\n",
        "        labels = []\n",
        "\n",
        "        for label_folder, label_value in zip([\"positive\", \"negative\"], [1, 0]):\n",
        "            spectrogram_folder = '/content/gdrive/MyDrive/RespiraCheck/cough_data/spectrograms_houman'\n",
        "            output_dir = os.path.join(spectrogram_folder, label_folder)\n",
        "\n",
        "            for image_name in tqdm(os.listdir(output_dir)):\n",
        "                image_path = os.path.join(output_dir, image_name)\n",
        "                image_tensor = self.image_to_tensor(image_path)\n",
        "\n",
        "                tensors.append(image_tensor)\n",
        "                labels.append(label_value)\n",
        "\n",
        "        # Tensor of all features (N x D) - N is number of samples (377), D is feature dimension (3,224,224)\n",
        "        X = torch.stack(tensors)\n",
        "        # Tensor of all labels (N x 1) - 377x1\n",
        "        y = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "        return TensorDataset(X, y)\n",
        "\n",
        "\n",
        "    def image_to_tensor(self, image_path: str) -> torch.Tensor:\n",
        "        \"\"\"Converts a spectrogram image to a PyTorch tensor.\n",
        "\n",
        "        Args:\n",
        "            image_path (str): Path to the spectrogram image file.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The PyTorch tensor representation of the image.\n",
        "        \"\"\"\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),  # Resize to ResNet18 input size\n",
        "            transforms.ToTensor(),  # Convert image to tensor\n",
        "        ])\n",
        "\n",
        "        image = Image.open(image_path).convert(\"RGB\") # Convert from RGBA to RGB\n",
        "        tensor_image = transform(image)\n",
        "\n",
        "        return tensor_image  # shape will be 3, 224, 224\n",
        "\n",
        "    def create_dataloaders(self, batch_size, dataset_path = None, upsample = True) -> tuple[DataLoader, DataLoader, DataLoader]:\n",
        "        \"\"\"Splits the dataset into training and test sets.\n",
        "\n",
        "        Args:\n",
        "            batch_size (int): The batch size for the DataLoader.\n",
        "            dataset_path (str | None): Path to the TensorDataset file.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (train_df, test_df) - The training and testing DataFrames.\n",
        "        \"\"\"\n",
        "        if dataset_path:\n",
        "            print(f\"Loading dataset from {dataset_path}\")\n",
        "            dataset = torch.load(dataset_path, weights_only=False)\n",
        "        else:\n",
        "            print(\"Processing and loading dataset\")\n",
        "            dataset = self.load_dataset()\n",
        "\n",
        "        # Calculate sizes\n",
        "        test_size = round(self.test_size * len(dataset))\n",
        "        val_size = round(self.val_size * len(dataset))\n",
        "        train_size = round(len(dataset) - test_size - val_size)  # Remaining for training\n",
        "\n",
        "        # Perform split\n",
        "        train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "        # Upsample positive class\n",
        "        if upsample:\n",
        "            print(\"Upsampling data\")\n",
        "            labels = [label.item() for _, label in train_dataset]\n",
        "            train_counts = {}\n",
        "            for label in labels:\n",
        "                train_counts[label] = train_counts.get(label, 0) + 1\n",
        "            # print(train_counts)\n",
        "\n",
        "            weights = torch.where(torch.tensor(labels) == 0, 1 / train_counts[0], 1 / train_counts[1])\n",
        "            # print(labels[:5], weights[:5])\n",
        "\n",
        "            wr_sampler = WeightedRandomSampler(weights, int(len(train_dataset) * 1.5))\n",
        "\n",
        "            train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=wr_sampler)\n",
        "\n",
        "        else:\n",
        "            print(\"No upsampling\")\n",
        "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Create DataLoaders\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Count labels in train_loader\n",
        "        train_counts = {}\n",
        "        for _, labels in train_loader:\n",
        "            for label in labels:\n",
        "                train_counts[label.item()] = train_counts.get(label.item(), 0) + 1\n",
        "\n",
        "        # print(train_counts)\n",
        "\n",
        "        # Reduce memory footprint\n",
        "        dataset, train_dataset, val_dataset, test_dataset = None, None, None, None\n",
        "\n",
        "        return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "k3JgY0lnclQH"
      },
      "outputs": [],
      "source": [
        "import torch.optim as opt\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# Static hyperparameters\n",
        "EPOCHS = 20\n",
        "\n",
        "# Learning rate scheduler\n",
        "STEPS_PER_LR_DECAY = 20\n",
        "LR_DECAY = 0.5\n",
        "\n",
        "# Model parameters\n",
        "DROPOUT = 0.5\n",
        "\n",
        "# Training\n",
        "LOSS_FN = nn.BCEWithLogitsLoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rgLkS_upcm6n"
      },
      "outputs": [],
      "source": [
        "model = CNN_CBAMModel(DROPOUT)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # Should return True\n",
        "print(torch.cuda.device_count())  # Should be 1 or more\n",
        "print(torch.cuda.get_device_name(0))  # Should print your GPU model"
      ],
      "metadata": {
        "id": "cM5R-MQLy3XH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ce642a4-8996-4774-f885-8ac1a94e8349"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "1\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define fixed hyperparameters\n",
        "batch_size = 16  # Reduced batch size for better generalization\n",
        "learning_rate = 0.0002  # More stable learning rate\n",
        "weight_decay = 5e-4  # Regularization strength\n",
        "dropout_rate = 0.6  # Increased dropout to reduce overfitting\n",
        "patience = 15  # Increased patience for early stopping\n",
        "epochs = 20  # Define EPOCHS\n",
        "\n",
        "datapipeline = DataPipeline(test_size=0.15, val_size=0.15)\n",
        "train_loader, val_loader, test_loader = datapipeline.create_dataloaders(batch_size=14, upsample=True)\n",
        "datapipeline = None"
      ],
      "metadata": {
        "id": "NMF2F9mZ0vhG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b117526f-2502-4aef-d621-193347a14cf4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing and loading dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1409/1409 [00:32<00:00, 43.18it/s]\n",
            "100%|██████████| 4274/4274 [01:48<00:00, 39.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upsampling data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXMzHuqedyuC",
        "outputId": "19373dcc-ddef-47e0-98b4-a6d374628d86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Training with batch size: 16, learning rate: 0.0002\n",
            "\n",
            "📊 Dataset Split:\n",
            "- Training Samples: 3979\n",
            "- Validation Samples: 852\n",
            "- Test Samples: 852\n",
            "\n",
            "🔄 Epoch 1/20\n",
            "0:\n",
            "LR: 0.0002\n",
            "Loss - 0.69311 | Accuracy - 50.10%\n",
            "VLoss - 0.69121 | VAccuracy - 59.11%\n",
            "\n",
            "Validation accuracy: 59.15% | Validation loss: 0.6912\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/cough_data/model_batch_size:16_lr:0.0002_1743901058.0055115.pth\n",
            "✅ New best validation loss: 0.6912 | Validation accuracy: 59.15%\n",
            "\n",
            "🔄 Epoch 2/20\n",
            "0:\n",
            "LR: 0.00019876883405951377\n",
            "Loss - 0.69297 | Accuracy - 50.17%\n",
            "VLoss - 0.69057 | VAccuracy - 62.41%\n",
            "\n",
            "Validation accuracy: 62.44% | Validation loss: 0.6906\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/cough_data/model_batch_size:16_lr:0.0002_1743901091.9868054.pth\n",
            "✅ New best validation loss: 0.6906 | Validation accuracy: 62.44%\n",
            "\n",
            "🔄 Epoch 3/20\n",
            "0:\n",
            "LR: 0.00019510565162951537\n",
            "Loss - 0.69206 | Accuracy - 53.22%\n",
            "VLoss - 0.68863 | VAccuracy - 66.28%\n",
            "\n",
            "Validation accuracy: 66.31% | Validation loss: 0.6886\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/cough_data/model_batch_size:16_lr:0.0002_1743901127.8857412.pth\n",
            "✅ New best validation loss: 0.6886 | Validation accuracy: 66.31%\n",
            "\n",
            "🔄 Epoch 4/20\n",
            "0:\n",
            "LR: 0.0001891006524188368\n",
            "Loss - 0.69192 | Accuracy - 52.68%\n",
            "VLoss - 0.68726 | VAccuracy - 66.14%\n",
            "\n",
            "Validation accuracy: 66.20% | Validation loss: 0.6873\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/cough_data/model_batch_size:16_lr:0.0002_1743901166.4694815.pth\n",
            "✅ New best validation loss: 0.6873 | Validation accuracy: 66.20%\n",
            "\n",
            "🔄 Epoch 5/20\n",
            "0:\n",
            "LR: 0.00018090169943749476\n",
            "Loss - 0.69087 | Accuracy - 54.16%\n",
            "VLoss - 0.69115 | VAccuracy - 56.30%\n",
            "\n",
            "Validation accuracy: 56.34% | Validation loss: 0.6911\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/cough_data/model_batch_size:16_lr:0.0002_1743901200.8993318.pth\n",
            "🔄 No improvement in validation loss for 1 epochs\n",
            "\n",
            "🔄 Epoch 6/20\n",
            "0:\n",
            "LR: 0.00017071067811865476\n",
            "Loss - 0.68972 | Accuracy - 55.30%\n",
            "VLoss - 0.69139 | VAccuracy - 55.25%\n",
            "\n",
            "Validation accuracy: 55.28% | Validation loss: 0.6914\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/cough_data/model_batch_size:16_lr:0.0002_1743901240.8495939.pth\n",
            "🔄 No improvement in validation loss for 2 epochs\n",
            "\n",
            "🔄 Epoch 7/20\n",
            "0:\n",
            "LR: 0.00015877852522924732\n",
            "Loss - 0.68638 | Accuracy - 56.88%\n",
            "VLoss - 0.68513 | VAccuracy - 59.97%\n",
            "\n",
            "Validation accuracy: 59.98% | Validation loss: 0.6851\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/cough_data/model_batch_size:16_lr:0.0002_1743901279.6109147.pth\n",
            "✅ New best validation loss: 0.6851 | Validation accuracy: 59.98%\n",
            "\n",
            "🔄 Epoch 8/20\n",
            "0:\n",
            "LR: 0.00014539904997395468\n",
            "Loss - 0.68338 | Accuracy - 57.74%\n",
            "VLoss - 0.68279 | VAccuracy - 59.15%\n",
            "\n",
            "Validation accuracy: 59.15% | Validation loss: 0.6828\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/cough_data/model_batch_size:16_lr:0.0002_1743901318.647066.pth\n",
            "✅ New best validation loss: 0.6828 | Validation accuracy: 59.15%\n",
            "\n",
            "🔄 Epoch 9/20\n",
            "0:\n",
            "LR: 0.00013090169943749476\n",
            "Loss - 0.68186 | Accuracy - 57.61%\n",
            "VLoss - 0.67650 | VAccuracy - 62.67%\n",
            "\n",
            "Validation accuracy: 62.68% | Validation loss: 0.6765\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/cough_data/model_batch_size:16_lr:0.0002_1743901357.2815866.pth\n",
            "✅ New best validation loss: 0.6765 | Validation accuracy: 62.68%\n",
            "\n",
            "🔄 Epoch 10/20\n",
            "0:\n",
            "LR: 0.00011564344650402312\n",
            "Loss - 0.67681 | Accuracy - 58.84%\n",
            "VLoss - 0.66999 | VAccuracy - 63.49%\n",
            "\n",
            "Validation accuracy: 63.50% | Validation loss: 0.6700\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/cough_data/model_batch_size:16_lr:0.0002_1743901394.4512968.pth\n",
            "✅ New best validation loss: 0.6700 | Validation accuracy: 63.50%\n",
            "\n",
            "🔄 Epoch 11/20\n",
            "0:\n",
            "LR: 0.00010000000000000002\n",
            "Loss - 0.67525 | Accuracy - 59.00%\n",
            "VLoss - 0.66644 | VAccuracy - 64.21%\n",
            "\n",
            "Validation accuracy: 64.20% | Validation loss: 0.6664\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/cough_data/model_batch_size:16_lr:0.0002_1743901431.5144832.pth\n",
            "✅ New best validation loss: 0.6664 | Validation accuracy: 64.20%\n",
            "\n",
            "🔄 Epoch 12/20\n",
            "0:\n",
            "LR: 8.435655349597696e-05\n",
            "Loss - 0.67150 | Accuracy - 59.52%\n",
            "VLoss - 0.64998 | VAccuracy - 67.70%\n",
            "\n",
            "Validation accuracy: 67.72% | Validation loss: 0.6500\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/cough_data/model_batch_size:16_lr:0.0002_1743901469.9958935.pth\n",
            "✅ New best validation loss: 0.6500 | Validation accuracy: 67.72%\n",
            "\n",
            "🔄 Epoch 13/20\n",
            "0:\n",
            "LR: 6.909830056250529e-05\n",
            "Loss - 0.66834 | Accuracy - 59.49%\n",
            "VLoss - 0.66748 | VAccuracy - 62.33%\n",
            "\n",
            "Validation accuracy: 62.32% | Validation loss: 0.6675\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/cough_data/model_batch_size:16_lr:0.0002_1743901508.8584054.pth\n",
            "🔄 No improvement in validation loss for 1 epochs\n",
            "\n",
            "🔄 Epoch 14/20\n",
            "0:\n",
            "LR: 5.460095002604534e-05\n",
            "Loss - 0.66865 | Accuracy - 59.10%\n",
            "VLoss - 0.65881 | VAccuracy - 64.44%\n",
            "\n",
            "Validation accuracy: 64.44% | Validation loss: 0.6588\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/cough_data/model_batch_size:16_lr:0.0002_1743901543.3948379.pth\n",
            "🔄 No improvement in validation loss for 2 epochs\n",
            "\n",
            "🔄 Epoch 15/20\n",
            "0:\n",
            "LR: 4.122147477075271e-05\n",
            "Loss - 0.66745 | Accuracy - 59.08%\n",
            "VLoss - 0.65541 | VAccuracy - 64.66%\n",
            "\n",
            "Validation accuracy: 64.67% | Validation loss: 0.6554\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/cough_data/model_batch_size:16_lr:0.0002_1743901580.4140031.pth\n",
            "🔄 No improvement in validation loss for 3 epochs\n",
            "\n",
            "🔄 Epoch 16/20\n",
            "0:\n",
            "LR: 2.928932188134526e-05\n",
            "Loss - 0.66863 | Accuracy - 58.92%\n",
            "VLoss - 0.65670 | VAccuracy - 64.32%\n",
            "\n",
            "Validation accuracy: 64.32% | Validation loss: 0.6567\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/cough_data/model_batch_size:16_lr:0.0002_1743901619.458415.pth\n",
            "🔄 No improvement in validation loss for 4 epochs\n",
            "\n",
            "🔄 Epoch 17/20\n",
            "0:\n",
            "LR: 1.909830056250527e-05\n",
            "Loss - 0.66774 | Accuracy - 60.00%\n",
            "VLoss - 0.64389 | VAccuracy - 67.23%\n",
            "\n",
            "Validation accuracy: 67.25% | Validation loss: 0.6439\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/cough_data/model_batch_size:16_lr:0.0002_1743901658.3108292.pth\n",
            "✅ New best validation loss: 0.6439 | Validation accuracy: 67.25%\n",
            "\n",
            "🔄 Epoch 18/20\n",
            "0:\n",
            "LR: 1.0899347581163223e-05\n",
            "Loss - 0.66660 | Accuracy - 59.70%\n",
            "VLoss - 0.65004 | VAccuracy - 65.71%\n",
            "\n",
            "Validation accuracy: 65.73% | Validation loss: 0.6500\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/cough_data/model_batch_size:16_lr:0.0002_1743901696.9946637.pth\n",
            "🔄 No improvement in validation loss for 1 epochs\n",
            "\n",
            "🔄 Epoch 19/20\n",
            "0:\n",
            "LR: 4.8943483704846475e-06\n",
            "Loss - 0.66406 | Accuracy - 60.51%\n",
            "VLoss - 0.65940 | VAccuracy - 64.31%\n",
            "\n",
            "Validation accuracy: 64.32% | Validation loss: 0.6594\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/cough_data/model_batch_size:16_lr:0.0002_1743901732.0189135.pth\n",
            "🔄 No improvement in validation loss for 2 epochs\n",
            "\n",
            "🔄 Epoch 20/20\n",
            "0:\n",
            "LR: 6.155829702431171e-07\n",
            "Loss - 0.66564 | Accuracy - 59.89%\n",
            "VLoss - 0.64833 | VAccuracy - 65.83%\n",
            "\n",
            "Validation accuracy: 65.85% | Validation loss: 0.6483\n",
            "Best model saved at: /content/gdrive/MyDrive/RespiraCheck/cough_data/model_batch_size:16_lr:0.0002_1743901769.3595529.pth\n",
            "🔄 No improvement in validation loss for 3 epochs\n",
            "\n",
            "🎯 Test accuracy: 60.91% 🚀 Best model saved!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as opt\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torchaudio.transforms as T\n",
        "import torch.nn.utils\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Loss function\n",
        "LOSS_FN = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Learning rate decay\n",
        "STEPS_PER_LR_DECAY = 20\n",
        "LR_DECAY = 0.5\n",
        "\n",
        "print(f\"\\n🚀 Training with batch size: {batch_size}, learning rate: {learning_rate}\")\n",
        "\n",
        "# Initialize model (Use custom CNNModel with EfficientNet-B0 + CBAM)\n",
        "cnn_model = CNN_CBAMModel(dropout=dropout_rate)\n",
        "\n",
        "# Choose optimizer (SGD with momentum for better generalization)\n",
        "optimizer = opt.SGD(params=cnn_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
        "\n",
        "# Learning rate scheduler (CosineAnnealingLR for smooth LR decay)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "# Data Augmentation (SpecAugment with additional Mixup)\n",
        "def augment_spectrogram(spectrogram):\n",
        "    spectrogram = T.FrequencyMasking(freq_mask_param=15)(spectrogram)\n",
        "    spectrogram = T.TimeMasking(time_mask_param=25)(spectrogram)\n",
        "    spectrogram = T.Vol(0.8)(spectrogram)  # Random volume adjustment\n",
        "    return spectrogram\n",
        "\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    index = torch.randperm(x.size(0))\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "# Create ModelHandler\n",
        "model_handler = ModelHandler(\n",
        "    model=cnn_model,\n",
        "    model_path=\"/content/gdrive/MyDrive/RespiraCheck/cough_data\",\n",
        "    optimizer=optimizer,\n",
        "    loss_function=LOSS_FN,\n",
        "    steps_per_decay=STEPS_PER_LR_DECAY,\n",
        "    lr_decay=LR_DECAY\n",
        ")\n",
        "\n",
        "# Debug: Check dataset distribution\n",
        "print(\"\\n📊 Dataset Split:\")\n",
        "print(f\"- Training Samples: {len(train_loader.dataset)}\")\n",
        "print(f\"- Validation Samples: {len(val_loader.dataset)}\")\n",
        "print(f\"- Test Samples: {len(test_loader.dataset)}\")\n",
        "\n",
        "# Training loop with early stopping\n",
        "best_val_loss = float(\"inf\")\n",
        "best_model = None\n",
        "best_acc = 0.0\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\n🔄 Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    # Train\n",
        "    train_results, val_results = model_handler.train(train_loader=train_loader, epochs=1, model_name=\"CNN_CBAM_EfficientNet\")\n",
        "    train_loss = train_results[\"loss\"][-1]  # Get the last recorded loss value\n",
        "\n",
        "    # Validate\n",
        "    val_acc, val_loss = model_handler.validate(val_loader, {\"batch_size\": batch_size, \"lr\": learning_rate})\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step()\n",
        "\n",
        "    # Gradient Clipping to stabilize training\n",
        "    torch.nn.utils.clip_grad_norm_(cnn_model.parameters(), max_norm=1.0)\n",
        "\n",
        "    # Check if validation loss improved\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_acc = val_acc\n",
        "        best_model = model_handler\n",
        "        epochs_without_improvement = 0  # Reset counter if there's improvement\n",
        "        print(f\"✅ New best validation loss: {best_val_loss:.4f} | Validation accuracy: {val_acc*100:.2f}%\")\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        print(f\"🔄 No improvement in validation loss for {epochs_without_improvement} epochs\")\n",
        "\n",
        "    # Early Stopping: Stop training if there's no improvement for `patience` epochs\n",
        "    if epochs_without_improvement >= patience:\n",
        "        print(f\"⏹️ Early stopping triggered due to no improvement in validation loss.\")\n",
        "        break\n",
        "\n",
        "# Final Testing\n",
        "if best_model:\n",
        "    test_acc = best_model.evaluate(test_loader)\n",
        "    print(f\"\\n🎯 Test accuracy: {test_acc*100:.2f}% 🚀 Best model saved!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwlwx85BhN3s",
        "outputId": "8f845613-088b-49b1-acd9-48214aa5c3d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Training on cuda with batch size: 16, learning rate: 0.0002\n",
            "\n",
            "📊 Dataset Split:\n",
            "- Training Samples: 3979\n",
            "- Validation Samples: 852\n",
            "- Test Samples: 852\n",
            "\n",
            "🔄 Epoch 1/20\n",
            "0:\n",
            "LR: 0.0002\n",
            "Loss - 0.00000 | Accuracy - 48.58%\n",
            "VLoss - 0.00000 | VAccuracy - 54.74%\n",
            "\n",
            "📊 Validation Accuracy: 73.12% | F1-score: 0.0000\n",
            "🔄 No improvement in F1-score for 1 epochs\n",
            "\n",
            "🔄 Epoch 2/20\n",
            "0:\n",
            "LR: 0.00019876883405951377\n",
            "Loss - 0.00000 | Accuracy - 49.05%\n",
            "VLoss - 0.00000 | VAccuracy - 51.70%\n",
            "\n",
            "📊 Validation Accuracy: 73.12% | F1-score: 0.0000\n",
            "🔄 No improvement in F1-score for 2 epochs\n",
            "\n",
            "🔄 Epoch 3/20\n",
            "0:\n",
            "LR: 0.00019510565162951537\n",
            "Loss - 0.00000 | Accuracy - 48.71%\n",
            "VLoss - 0.00000 | VAccuracy - 55.41%\n",
            "\n",
            "📊 Validation Accuracy: 73.12% | F1-score: 0.0000\n",
            "🔄 No improvement in F1-score for 3 epochs\n",
            "\n",
            "🔄 Epoch 4/20\n",
            "0:\n",
            "LR: 0.0001891006524188368\n",
            "Loss - 0.00000 | Accuracy - 48.75%\n",
            "VLoss - 0.00000 | VAccuracy - 55.17%\n",
            "\n",
            "📊 Validation Accuracy: 73.12% | F1-score: 0.0000\n",
            "🔄 No improvement in F1-score for 4 epochs\n",
            "\n",
            "🔄 Epoch 5/20\n",
            "0:\n",
            "LR: 0.00018090169943749476\n",
            "Loss - 0.00000 | Accuracy - 48.34%\n",
            "VLoss - 0.00000 | VAccuracy - 58.00%\n",
            "\n",
            "📊 Validation Accuracy: 73.12% | F1-score: 0.0000\n",
            "🔄 No improvement in F1-score for 5 epochs\n",
            "\n",
            "🔄 Epoch 6/20\n",
            "0:\n",
            "LR: 0.00017071067811865476\n",
            "Loss - 0.00000 | Accuracy - 49.31%\n",
            "VLoss - 0.00000 | VAccuracy - 53.32%\n",
            "\n",
            "📊 Validation Accuracy: 73.12% | F1-score: 0.0000\n",
            "🔄 No improvement in F1-score for 6 epochs\n",
            "\n",
            "🔄 Epoch 7/20\n",
            "0:\n",
            "LR: 0.00015877852522924732\n",
            "Loss - 0.00000 | Accuracy - 49.28%\n",
            "VLoss - 0.00000 | VAccuracy - 55.21%\n",
            "\n",
            "📊 Validation Accuracy: 73.12% | F1-score: 0.0000\n",
            "🔄 No improvement in F1-score for 7 epochs\n",
            "\n",
            "🔄 Epoch 8/20\n",
            "0:\n",
            "LR: 0.00014539904997395468\n",
            "Loss - 0.00000 | Accuracy - 49.57%\n",
            "VLoss - 0.00000 | VAccuracy - 55.78%\n",
            "\n",
            "📊 Validation Accuracy: 73.12% | F1-score: 0.0000\n",
            "🔄 No improvement in F1-score for 8 epochs\n",
            "\n",
            "🔄 Epoch 9/20\n",
            "0:\n",
            "LR: 0.00013090169943749476\n",
            "Loss - 0.00000 | Accuracy - 49.11%\n",
            "VLoss - 0.00000 | VAccuracy - 53.55%\n",
            "\n",
            "📊 Validation Accuracy: 73.12% | F1-score: 0.0000\n",
            "🔄 No improvement in F1-score for 9 epochs\n",
            "\n",
            "🔄 Epoch 10/20\n",
            "0:\n",
            "LR: 0.00011564344650402312\n",
            "Loss - 0.00000 | Accuracy - 49.38%\n",
            "VLoss - 0.00000 | VAccuracy - 51.44%\n",
            "\n",
            "📊 Validation Accuracy: 73.12% | F1-score: 0.0000\n",
            "🔄 No improvement in F1-score for 10 epochs\n",
            "\n",
            "🔄 Epoch 11/20\n",
            "0:\n",
            "LR: 0.00010000000000000002\n",
            "Loss - 0.00000 | Accuracy - 49.92%\n",
            "VLoss - 0.00000 | VAccuracy - 52.03%\n",
            "\n",
            "📊 Validation Accuracy: 73.12% | F1-score: 0.0000\n",
            "🔄 No improvement in F1-score for 11 epochs\n",
            "\n",
            "🔄 Epoch 12/20\n",
            "0:\n",
            "LR: 8.435655349597696e-05\n",
            "Loss - 0.00000 | Accuracy - 49.20%\n",
            "VLoss - 0.00000 | VAccuracy - 49.92%\n",
            "\n",
            "📊 Validation Accuracy: 73.12% | F1-score: 0.0000\n",
            "🔄 No improvement in F1-score for 12 epochs\n",
            "\n",
            "🔄 Epoch 13/20\n",
            "0:\n",
            "LR: 6.909830056250529e-05\n",
            "Loss - 0.00000 | Accuracy - 49.77%\n",
            "VLoss - 0.00000 | VAccuracy - 55.56%\n",
            "\n",
            "📊 Validation Accuracy: 73.12% | F1-score: 0.0000\n",
            "🔄 No improvement in F1-score for 13 epochs\n",
            "\n",
            "🔄 Epoch 14/20\n",
            "0:\n",
            "LR: 5.460095002604534e-05\n",
            "Loss - 0.00000 | Accuracy - 48.45%\n",
            "VLoss - 0.00000 | VAccuracy - 55.66%\n",
            "\n",
            "📊 Validation Accuracy: 73.12% | F1-score: 0.0000\n",
            "🔄 No improvement in F1-score for 14 epochs\n",
            "\n",
            "🔄 Epoch 15/20\n",
            "0:\n",
            "LR: 4.122147477075271e-05\n",
            "Loss - 0.00000 | Accuracy - 49.24%\n",
            "VLoss - 0.00000 | VAccuracy - 52.81%\n",
            "\n",
            "📊 Validation Accuracy: 73.12% | F1-score: 0.0000\n",
            "🔄 No improvement in F1-score for 15 epochs\n",
            "⏹️ Early stopping triggered due to no improvement in F1-score.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as opt\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torchaudio.transforms as T\n",
        "import torch.nn.utils\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "# Define fixed hyperparameters\n",
        "batch_size = 16  # Reduced batch size for better generalization\n",
        "learning_rate = 0.0002  # More stable learning rate\n",
        "weight_decay = 5e-4  # Regularization strength\n",
        "dropout_rate = 0.6  # Increased dropout to reduce overfitting\n",
        "patience = 15  # Early stopping patience\n",
        "epochs = 20  # Define total epochs\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\\n🚀 Training on {device} with batch size: {batch_size}, learning rate: {learning_rate}\")\n",
        "\n",
        "# Initialize model (Use custom CNNModel with EfficientNet-B0 + CBAM)\n",
        "cnn_model = CNN_CBAMModel(dropout=dropout_rate).to(device)  # Move model to GPU/CPU\n",
        "\n",
        "# Loss function\n",
        "LOSS_FN = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Learning rate decay\n",
        "STEPS_PER_LR_DECAY = 20\n",
        "LR_DECAY = 0.5\n",
        "\n",
        "# Choose optimizer (SGD with momentum for better generalization)\n",
        "optimizer = opt.SGD(params=cnn_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
        "\n",
        "# Learning rate scheduler (CosineAnnealingLR for smooth LR decay)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "# Data Augmentation (SpecAugment + Mixup)\n",
        "def augment_spectrogram(spectrogram):\n",
        "    # Apply Log-Mel Spectrogram transformation here\n",
        "    spectrogram = compute_log_mel_spectrogram(spectrogram)\n",
        "\n",
        "    # SpecAugment augmentations\n",
        "    spectrogram = T.FrequencyMasking(freq_mask_param=15)(spectrogram)  # Frequency Masking\n",
        "    spectrogram = T.TimeMasking(time_mask_param=25)(spectrogram)  # Time Masking\n",
        "\n",
        "    return spectrogram\n",
        "\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    index = torch.randperm(x.size(0))\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "# Create ModelHandler\n",
        "model_handler = ModelHandler(\n",
        "    model=cnn_model,\n",
        "    model_path=\"/content/gdrive/MyDrive/RespiraCheck/cough_data\",\n",
        "    optimizer=optimizer,\n",
        "    loss_function=LOSS_FN,\n",
        "    steps_per_decay=STEPS_PER_LR_DECAY,\n",
        "    lr_decay=LR_DECAY\n",
        ")\n",
        "\n",
        "# Debug: Check dataset distribution\n",
        "print(\"\\n📊 Dataset Split:\")\n",
        "print(f\"- Training Samples: {len(train_loader.dataset)}\")\n",
        "print(f\"- Validation Samples: {len(val_loader.dataset)}\")\n",
        "print(f\"- Test Samples: {len(test_loader.dataset)}\")\n",
        "\n",
        "# Function to Validate and Compute F1-score\n",
        "def validate(model_handler, val_loader):\n",
        "    model_handler.model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.to(device), targets.to(device)  # Move to GPU/CPU\n",
        "            outputs = model_handler.model(inputs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    val_acc = accuracy_score(all_targets, all_preds)\n",
        "    val_f1 = f1_score(all_targets, all_preds, average=\"binary\")\n",
        "\n",
        "    return val_acc, val_f1\n",
        "\n",
        "# Training loop with early stopping\n",
        "best_f1_score = 0.0\n",
        "best_model = None\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\n🔄 Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    # Train\n",
        "    train_results, val_results = model_handler.train(train_loader=train_loader, epochs=1, model_name=\"CNN_CBAM_EfficientNet\")\n",
        "\n",
        "    # Validate and Compute F1-score\n",
        "    val_acc, val_f1 = validate(model_handler, val_loader)\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"📊 Validation Accuracy: {val_acc*100:.2f}% | F1-score: {val_f1:.4f}\")\n",
        "\n",
        "    # Check for improvement\n",
        "    if val_f1 > best_f1_score:\n",
        "        best_f1_score = val_f1\n",
        "        best_model = model_handler\n",
        "        epochs_without_improvement = 0  # Reset counter if there's improvement\n",
        "        print(f\"✅ New best F1-score: {best_f1_score:.4f}\")\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        print(f\"🔄 No improvement in F1-score for {epochs_without_improvement} epochs\")\n",
        "\n",
        "    # Early Stopping\n",
        "    if epochs_without_improvement >= patience:\n",
        "        print(f\"⏹️ Early stopping triggered due to no improvement in F1-score.\")\n",
        "        break\n",
        "\n",
        "# Function to Evaluate Model on Test Set\n",
        "def evaluate(model_handler, test_loader):\n",
        "    model_handler.model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.to(device), targets.to(device)  # Move to GPU/CPU\n",
        "            outputs = model_handler.model(inputs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    test_acc = accuracy_score(all_targets, all_preds)\n",
        "    test_f1 = f1_score(all_targets, all_preds, average=\"binary\")\n",
        "\n",
        "    print(f\"\\n🎯 Test Accuracy: {test_acc*100:.2f}% | Test F1-score: {test_f1:.4f} 🚀\")\n",
        "    return test_acc, test_f1\n",
        "\n",
        "# Final Testing\n",
        "if best_model:\n",
        "    test_acc, test_f1 = evaluate(best_model, test_loader)\n",
        "    print(f\"\\n🎯 Test Accuracy: {test_acc*100:.2f}% | Test F1-score: {test_f1:.4f} 🚀 Best model saved!\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U8yaZDlMAuXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BCJWTeWXBLWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-bNu5asYBK4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as opt\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torchaudio.transforms as T\n",
        "import torch.nn.utils\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Fixed hyperparameters\n",
        "batch_size = 16\n",
        "learning_rate = 0.0002\n",
        "weight_decay = 5e-4\n",
        "dropout_rate = 0.6\n",
        "patience = 15\n",
        "EPOCHS = 20\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\\n🚀 Training with batch size: {batch_size}, learning rate: {learning_rate}\")\n",
        "\n",
        "# Model init\n",
        "cnn_model = CNN_CBAMModel(dropout=dropout_rate).to(device)\n",
        "optimizer = torch.optim.AdamW(cnn_model.parameters(), lr=0.0003, weight_decay=weight_decay)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=20)\n",
        "\n",
        "# Log-Mel Spectrogram transformation\n",
        "sample_rate = 16000\n",
        "n_fft = 400\n",
        "win_length = 400\n",
        "hop_length = 160\n",
        "n_mels = 23\n",
        "f_min = 0\n",
        "f_max = sample_rate // 2\n",
        "\n",
        "log_mel_transform = T.MelSpectrogram(\n",
        "    sample_rate=sample_rate,\n",
        "    n_fft=n_fft,\n",
        "    win_length=win_length,\n",
        "    hop_length=hop_length,\n",
        "    n_mels=n_mels,\n",
        "    f_min=f_min,\n",
        "    f_max=f_max\n",
        ")\n",
        "log_transform = T.AmplitudeToDB(stype='power')\n",
        "\n",
        "def compute_log_mel_spectrogram(audio_waveform):\n",
        "    mel_spectrogram = log_mel_transform(audio_waveform)\n",
        "    log_mel_spectrogram = log_transform(mel_spectrogram)\n",
        "    return log_mel_spectrogram\n",
        "\n",
        "def augment_spectrogram(spectrogram):\n",
        "    spectrogram = compute_log_mel_spectrogram(spectrogram)\n",
        "    spectrogram = T.FrequencyMasking(freq_mask_param=10)(spectrogram)\n",
        "    spectrogram = T.TimeMasking(time_mask_param=20)(spectrogram)\n",
        "    return spectrogram\n",
        "\n",
        "# Load data\n",
        "datapipeline = DataPipeline(test_size=0.15, val_size=0.15)\n",
        "train_loader, val_loader, test_loader, all_train_labels = datapipeline.create_dataloaders(batch_size=batch_size, upsample=True)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(all_train_labels), y=all_train_labels)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "\n",
        "# Define loss\n",
        "loss_function = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "# Initialize model handler\n",
        "model_handler = ModelHandler(\n",
        "    model=cnn_model,\n",
        "    model_path=\"/content/gdrive/MyDrive/RespiraCheck/Cough Data\",\n",
        "    optimizer=optimizer,\n",
        "    loss_function=loss_function,\n",
        "    steps_per_decay=STEPS_PER_LR_DECAY,\n",
        "    lr_decay=LR_DECAY\n",
        ")\n",
        "\n",
        "# Validation function\n",
        "def validate(model_handler, val_loader):\n",
        "    model_handler.model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model_handler.model(inputs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_targets = np.array(all_targets)\n",
        "\n",
        "    val_acc = accuracy_score(all_targets, all_preds)\n",
        "\n",
        "    tp = np.sum((all_preds == 1) & (all_targets == 1))\n",
        "    fp = np.sum((all_preds == 1) & (all_targets == 0))\n",
        "    fn = np.sum((all_preds == 0) & (all_targets == 1))\n",
        "    tn = np.sum((all_preds == 0) & (all_targets == 0))\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0.0\n",
        "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0.0\n",
        "    val_f1 = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0.0\n",
        "\n",
        "    print(f\"📊 Validation Accuracy: {val_acc*100:.2f}% | F1-score: {val_f1:.4f}\")\n",
        "    print(f\"TP: {tp} | FP: {fp} | FN: {fn} | TN: {tn} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "    return val_acc, val_f1\n",
        "\n",
        "# Evaluation\n",
        "def evaluate(model_handler, test_loader):\n",
        "    model_handler.model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model_handler.model(inputs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_targets = np.array(all_targets)\n",
        "\n",
        "    test_acc = accuracy_score(all_targets, all_preds)\n",
        "\n",
        "    tp = np.sum((all_preds == 1) & (all_targets == 1))\n",
        "    fp = np.sum((all_preds == 1) & (all_targets == 0))\n",
        "    fn = np.sum((all_preds == 0) & (all_targets == 1))\n",
        "    tn = np.sum((all_preds == 0) & (all_targets == 0))\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0.0\n",
        "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0.0\n",
        "    test_f1 = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0.0\n",
        "\n",
        "    print(f\"\\n🎯 Test Accuracy: {test_acc*100:.2f}% | Test F1-score: {test_f1:.4f} 🚀\")\n",
        "    print(f\"TP: {tp} | FP: {fp} | FN: {fn} | TN: {tn} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "    return test_acc, test_f1\n",
        "\n",
        "# Training loop\n",
        "best_f1_score = 0.0\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n🔄 Epoch {epoch+1}/{EPOCHS}\")\n",
        "   # for batch in train_loader:\n",
        "   #     inputs, targets = batch\n",
        "   #     inputs = augment_spectrogram(inputs).to(device)\n",
        "   #     targets = targets.to(device)\n",
        "\n",
        "    train_results, val_results = model_handler.train(train_loader=train_loader, val_loader=val_loader,epochs=1, model_name=\"CNN_EfficientNet\")\n",
        "    val_acc, val_f1 = validate(model_handler, val_loader)\n",
        "    scheduler.step()\n",
        "\n",
        "    if val_f1 > best_f1_score:\n",
        "        best_f1_score = val_f1\n",
        "        best_model = model_handler\n",
        "        epochs_without_improvement = 0\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "\n",
        "    if epochs_without_improvement >= patience:\n",
        "        print(f\"⏹️ Early stopping triggered due to no improvement in F1-score.\")\n",
        "        break\n",
        "\n",
        "# Final evaluation\n",
        "if best_model:\n",
        "    test_acc, test_f1 = evaluate(best_model, test_loader)\n",
        "    print(f\"\\n🎯 Test Accuracy: {test_acc*100:.2f}% | Test F1-score: {test_f1:.4f} 🚀 Best model saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "DMNdQFkWkN_p",
        "outputId": "64131ce5-df21-4f59-bcee-2f96ff5400e0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Training with batch size: 16, learning rate: 0.0002\n",
            "Processing and loading dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1409/1409 [00:29<00:00, 48.48it/s]\n",
            "100%|██████████| 4274/4274 [01:47<00:00, 39.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upsampling data\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 4, got 3)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ba0a286b853b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mdatapipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_train_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatapipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# Compute class weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}